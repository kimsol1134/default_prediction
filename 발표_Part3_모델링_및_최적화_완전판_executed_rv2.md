# ğŸ“— Part 3 v3 ì™„ì „íŒ

**Priority 1:** Ensemble+Statistical Test, n_iter=200  

**Priority 2:** Cumulative Gains, ì‹œê°í™”, CV ì„ê³„ê°’  

**Priority 3:** Winsorizer, Calibration, Learning Curve

## 0. í™˜ê²½ ì„¤ì •


```python
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import platform
if platform.system() == 'Darwin': plt.rc('font', family='AppleGothic')
elif platform.system() == 'Windows': plt.rc('font', family='Malgun Gothic')
else: plt.rc('font', family='NanumGothic')
plt.rc('axes', unicode_minus=False)
from tqdm.auto import tqdm

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict, RandomizedSearchCV, learning_curve
from sklearn.preprocessing import RobustScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import confusion_matrix, roc_auc_score,
average_precision_score, precision_recall_curve, fbeta_score, make_scorer
from sklearn.calibration import calibration_curve
from imblearn.over_sampling import SMOTE, BorderlineSMOTE
from imblearn.combine import SMOTETomek
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier,RandomForestClassifier, ExtraTreesClassifier
from sklearn.metrics import average_precision_score
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
from scipy.stats import wilcoxon
import joblib, os
from datetime import datetime
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
print('âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ')
```

    âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ



```python
DATA_DIR = '/Users/user/Desktop/ì•ˆì•Œë´ì¥¼/data/'
PROCESSED_DIR = os.path.join(DATA_DIR, 'processed')
os.makedirs(PROCESSED_DIR, exist_ok=True)
INPUT_FILE = os.path.join(DATA_DIR,'domain_based_features_ì™„ì „íŒ.csv')
OUTPUT_PREFIX = 'ë°œí‘œ_Part3_v3'
```

## 1. 3-Way Split (Train 60% / Val 20% / Test 20%)

### ğŸ›‘ ëª¨ë¸ë§ ì „ëµ í•µì‹¬ ë³€ê²½

#### 1. ë³€ìˆ˜ ì œê±°: `'ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜'` ğŸš«
* **ì‚¬ìœ :** ë¶€ë„ ë°œìƒê³¼ ë™ì‹œì— ë‚˜íƒ€ë‚˜ëŠ” í›„í–‰ì„± ì§€í‘œ(Data Leakage)ì…ë‹ˆë‹¤.
* **ê²°ì •:** ì´ë¥¼ ì œì™¸í•˜ì—¬ "ì´ë¯¸ ë§í•œ ê¸°ì—… íƒì§€"ê°€ ì•„ë‹Œ, **"ë§í•  ì§•í›„ë¥¼ ë¯¸ë¦¬ í¬ì°©(ì¡°ê¸° ê²½ë³´)"**í•˜ëŠ” ìˆœìˆ˜ ì¬ë¬´ ê¸°ë°˜ ëª¨ë¸ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.

#### 2. ë°ì´í„° ë¶„í•  ì „ëµ: 3-Way Split (60% : 20% : 20%) ğŸ“Š
* **Train (60%):** ë¶ˆê· í˜•í•œ ë°ì´í„°(1.5% ë¶€ë„ìœ¨)ì—ì„œ ì†Œìˆ˜ í´ë˜ìŠ¤ë¥¼ ì¶©ë¶„íˆ í•™ìŠµí•˜ê¸° ìœ„í•¨.
* **Valid (20%):** ëª¨ë¸ íŠœë‹(AutoML) ë° ìµœì  ì„ê³„ê°’(Threshold) ê²°ì •ì„ ìœ„í•œ ê²€ì¦ìš©.
* **Test (20%):** í•™ìŠµ ê³¼ì •ì— ì „í˜€ ê´€ì—¬í•˜ì§€ ì•Šì€ **ì™„ì „í•œ ë¯¸ì§€(Unseen) ë°ì´í„°**ë¡œ ìµœì¢… ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€.


```python
df = pd.read_csv(INPUT_FILE, encoding='utf-8')
TARGET_COL = 'ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'

X = df.drop(columns=[TARGET_COL])

# -------------------------------------------------------
# [ìˆ˜ì •] ë…¼ë¦¬ì  ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ 'ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜' ì œê±°
# -------------------------------------------------------
drop_cols = ['ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜']
existing_drop = [c for c in drop_cols if c in X.columns]
if existing_drop:
    X = X.drop(columns=existing_drop)
    print(f"ğŸš« ì œê±°ëœ ë³€ìˆ˜: {existing_drop}")
# -------------------------------------------------------

y = df[TARGET_COL]
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_STATE)
print(f'Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,}')
print('âš ï¸ TestëŠ” ìµœì¢… í‰ê°€ê¹Œì§€ ì‚¬ìš© ì•ˆ í•¨!')
```

    ğŸš« ì œê±°ëœ ë³€ìˆ˜: ['ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜']
    Train: 30,000 | Val: 10,000 | Test: 10,000
    âš ï¸ TestëŠ” ìµœì¢… í‰ê°€ê¹Œì§€ ì‚¬ìš© ì•ˆ í•¨!


## 2. ì „ì²˜ë¦¬ + â­ Winsorizer ì‹¤í—˜ (Priority 3)


```python
from sklearn.base import BaseEstimator, TransformerMixin

class InfiniteHandler(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None): return self
    def transform(self, X): return X.replace([np.inf, -np.inf], np.nan)

class LogTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, eps=1e-10): self.eps = eps
    def fit(self, X, y=None): return self
    def transform(self, X):
        X_c = X.copy()
        for c in X_c.columns:
            if (X_c[c] >= 0).all(): X_c[c] = np.log1p(X_c[c] + self.eps)
        return X_c

class Winsorizer(BaseEstimator, TransformerMixin):
    def __init__(self, l=0.005, u=0.995): self.l, self.u, self.b = l, u, {}
    def fit(self, X, y=None):
        for c in X.columns: self.b[c] = (X[c].quantile(self.l), X[c].quantile(self.u))
        return self
    def transform(self, X):
        X_c = X.copy()
        for c in X_c.columns: X_c[c] = X_c[c].clip(*self.b[c])
        return X_c
```

### ğŸ› ï¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° ì´ìƒì¹˜ ì œì–´ ì‹¤í—˜

ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ **í‘œì¤€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**ì„ ì •ì˜í•˜ê³ , **ì´ìƒì¹˜ ì œì–´(Winsorizer)** íš¨ê³¼ë¥¼ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

1.  **íŒŒì´í”„ë¼ì¸ êµ¬ì„±**:
    - `ê²°ì¸¡ì¹˜/ë¬´í•œëŒ€ ì²˜ë¦¬` $\rightarrow$ `ë¡œê·¸ ë³€í™˜(ì™œë„ ë³´ì •)` $\rightarrow$ `Robust Scaling` $\rightarrow$ `SMOTE(ì˜µì…˜)` ìˆœìœ¼ë¡œ ë°ì´í„° ì •ì œ.
    
2.  **Winsorizer ë„ì… ì‹¤í—˜**:
    - **ê°€ì„¤**: ì¬ë¬´ ë°ì´í„°ì˜ ê·¹ë‹¨ì  ì´ìƒì¹˜(Outlier)ë¥¼ ìƒ/í•˜ìœ„ 0.5%ë¡œ ì œí•œ(Clipping)í•˜ë©´ ì„±ëŠ¥ì´ ì˜¤ë¥¼ ê²ƒì´ë‹¤.
    - **ê²°ê³¼**: ì ìš© ì‹œ **PR-AUCê°€ í•˜ë½ (0.1284 $\rightarrow$ 0.1188)** í•¨ì„ í™•ì¸.
    - **ê²°ì •**: **Winsorizer ë¯¸ì ìš© (False)**. ì´ìƒì¹˜ì—ë„ ë¶€ë„ ì§•í›„ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜, `RobustScaler`ì™€ `Logë³€í™˜`ë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ì œì–´ë¨ì„ ì‹œì‚¬.


```python
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

def create_pipeline(clf, wins=False, resamp=None):
    s = [
        ('inf', InfiniteHandler()), 
        ('imp', SimpleImputer(strategy='median').set_output(transform='pandas')), 
        ('log', LogTransformer())
    ]
    if wins: s.append(('wins', Winsorizer()))
    s.append(('scaler', RobustScaler()))
    s.append(('resamp', SMOTE(sampling_strategy=0.2, random_state=RANDOM_STATE) if resamp else 'passthrough'))
    s.append(('clf', clf))
    return ImbPipeline(s)

lgbm_t = lgb.LGBMClassifier(n_estimators=100, max_depth=5, learning_rate=0.05, scale_pos_weight=scale_pos_weight, random_state=RANDOM_STATE, verbose=-1)
print('Winsorizer ì‹¤í—˜...')
wins_r = {}
for w in [False, True]:
    p = create_pipeline(lgbm_t, wins=w)
    p.fit(X_train, y_train)
    pr = average_precision_score(y_val, p.predict_proba(X_val)[:, 1])
    wins_r[w] = pr
    print(f'Wins={w}: PR-AUC={pr:.4f}')
USE_WINSORIZER = wins_r[True] > wins_r[False]
print(f'âœ… Winsorizer={USE_WINSORIZER}')
```

    Winsorizer ì‹¤í—˜...
    Wins=False: PR-AUC=0.1284
    Wins=True: PR-AUC=0.1188
    âœ… Winsorizer=False


## 3. ë¦¬ìƒ˜í”Œë§ ì „ëµ (SMOTE vs Class Weight)

### âš–ï¸ ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ ì „ëµ: SMOTE vs Class Weight

1:66ì˜ ê·¹ì‹¬í•œ ë°ì´í„° ë¶ˆê· í˜•ì„ í•´ì†Œí•˜ê¸° ìœ„í•´ **ë°ì´í„° ì¦ê°•(Resampling)**ê³¼ **ê°€ì¤‘ì¹˜ ì¡°ì •(Cost-sensitive Learning)** ë°©ì‹ì„ ë¹„êµ ì‹¤í—˜í–ˆìŠµë‹ˆë‹¤.

1.  **ì‹¤í—˜ ì„¤ê³„**:
    - **Resampling**: SMOTE ê³„ì—´ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì†Œìˆ˜ í´ë˜ìŠ¤(ë¶€ë„) ë°ì´í„°ë¥¼ ì¸ìœ„ì ìœ¼ë¡œ ìƒì„±í•˜ì—¬ í•™ìŠµ.
    - **Class Weight**: ë°ì´í„° ìƒì„± ì—†ì´, ë¶€ë„ ë°ì´í„° ì˜¤ë¶„ë¥˜ ì‹œ ëª¨ë¸ì— ë” í° ë²Œì (Penalty)ì„ ë¶€ê³¼í•˜ì—¬ í•™ìŠµ.

2.  **ì‹¤í—˜ ê²°ê³¼ ë° ê²°ì •**:
    - **SMOTE (0.1300)** > Class Weight (0.1284)
    - ë°ì´í„°ë¥¼ ì§ì ‘ ì¦ê°•í•˜ëŠ” ë°©ì‹ì´ ëª¨ë¸ì˜ ê²°ì • ê²½ê³„ë¥¼ ë” ëª…í™•í•˜ê²Œ í˜•ì„±í•¨ì´ ì…ì¦ë˜ì–´, **ìµœì¢…ì ìœ¼ë¡œ SMOTEë¥¼ ì±„íƒ**í–ˆìŠµë‹ˆë‹¤.


```python
lgbm_b = lgb.LGBMClassifier(n_estimators=100, max_depth=5, learning_rate=0.05, random_state=RANDOM_STATE, verbose=-1)
strat_a = {}
for v in ['smote', 'borderline', 'smote_tomek']:
    p = create_pipeline(lgbm_b, wins=USE_WINSORIZER, resamp=v)
    p.fit(X_train, y_train)
    pr = average_precision_score(y_val, p.predict_proba(X_val)[:, 1])
    strat_a[v] = pr
    print(f'{v}: {pr:.4f}')

lgbm_w = lgb.LGBMClassifier(n_estimators=100, max_depth=5, learning_rate=0.05, scale_pos_weight=scale_pos_weight, random_state=RANDOM_STATE, verbose=-1)
p_b = create_pipeline(lgbm_w, wins=USE_WINSORIZER)
p_b.fit(X_train, y_train)
strat_b_pr = average_precision_score(y_val, p_b.predict_proba(X_val)[:, 1])
print(f'Class Weight: {strat_b_pr:.4f}')

best_smote = max(strat_a, key=strat_a.get)
if strat_b_pr > strat_a[best_smote]:
    selected_strategy = 'Class Weight'
    selected_resampler = None
else:
    selected_strategy = f'SMOTE ({best_smote})'
    selected_resampler = best_smote
print(f'âœ… ì„ íƒ: {selected_strategy}')
```

    smote: 0.1300
    borderline: 0.1300
    smote_tomek: 0.1300
    Class Weight: 0.1284
    âœ… ì„ íƒ: SMOTE (smote)


## 4. â­ AutoML (Priority 1 - n_iter=200)

### ğŸ¤– AutoML: ìµœì  ì•Œê³ ë¦¬ì¦˜ ë° íŒŒë¼ë¯¸í„° íƒìƒ‰

ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜(Boosting, Bagging, Linear)ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìˆ˜í–‰í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ ë°œêµ´í–ˆìŠµë‹ˆë‹¤.

1.  **ëª¨ë¸ ë‹¤ì–‘ì„± í™•ë³´**:
    - **Boosting**: LightGBM, XGBoost, CatBoost (ê°•ë ¥í•œ ì˜ˆì¸¡ë ¥)
    - **Bagging**: RandomForest, ExtraTrees (ê³¼ì í•© ë°©ì§€ ë° ì•ˆì •ì„±)
    - **Linear**: LogisticRegression (Baseline ë° ì„¤ëª…ë ¥)

2.  **ìµœì í™” ì „ëµ**:
    - `RandomizedSearchCV`ë¥¼ í†µí•´ ëª¨ë¸ë³„ í•µì‹¬ íŒŒë¼ë¯¸í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰.
    - ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ ëª¨ë¸ íŠ¹ì„±ì— ë§ëŠ” ê°€ì¤‘ì¹˜(`scale_pos_weight`, `class_weight`)ë¥¼ ë™ì ìœ¼ë¡œ í• ë‹¹.

3.  **ì‹¤í—˜ ê²°ê³¼**:
    - **CatBoost (0.1444)** ê°€ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ì—¬ ìµœì¢… ëª¨ë¸ í›„ë³´ 1ìˆœìœ„ë¡œ ì„ ì •.


```python

# 1. ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜ (ê³µí†µ íŒŒë¼ë¯¸í„° ì œê±°í•˜ê³  ê°œë³„ ì •ì˜)
# -------------------------------------------------------------------------

# Boosting ê³„ì—´ (scale_pos_weight ì‚¬ìš©)
lgbm_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'num_leaves': [31, 63, 127],
    'min_child_samples': [20, 50, 100]
}

xgb_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.8, 0.9, 1.0]
}

cb_grid = {
    'iterations': [100, 200, 300],
    'depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'l2_leaf_reg': [1, 3, 5]
}

# Bagging & Linear ê³„ì—´ (class_weight ì‚¬ìš©)
rf_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

lr_grid = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['liblinear']
}

# 2. ê°€ì¤‘ì¹˜(Weight) ë™ì  í• ë‹¹ ë¡œì§
# -------------------------------------------------------------------------
if selected_resampler is None:
    # SMOTE ë¯¸ì‚¬ìš© ì‹œ: ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ ê°•í•œ ê°€ì¤‘ì¹˜ ì ìš©
    # Boosting ê³„ì—´ìš©
    boost_weights = [scale_pos_weight, scale_pos_weight * 1.5]
    lgbm_grid['scale_pos_weight'] = boost_weights
    xgb_grid['scale_pos_weight'] = boost_weights
    cb_grid['scale_pos_weight'] = boost_weights
    
    # RF, LR ê³„ì—´ìš©
    rf_grid['class_weight'] = ['balanced', 'balanced_subsample']
    lr_grid['class_weight'] = ['balanced']
else:
    # SMOTE ì‚¬ìš© ì‹œ: ê°€ì¤‘ì¹˜ 1 (ë˜ëŠ” None)
    # Boosting ê³„ì—´ìš©
    lgbm_grid['scale_pos_weight'] = [1]
    xgb_grid['scale_pos_weight'] = [1]
    cb_grid['scale_pos_weight'] = [1]
    
    # RF, LR ê³„ì—´ìš©
    rf_grid['class_weight'] = [None]
    lr_grid['class_weight'] = [None]

# 3. ëª¨ë¸ ì •ì˜
# -------------------------------------------------------------------------
models = {
    'LightGBM': (lgb.LGBMClassifier(random_state=RANDOM_STATE, verbose=-1), lgbm_grid),
    'XGBoost': (xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'), xgb_grid),
    'CatBoost': (CatBoostClassifier(random_state=RANDOM_STATE, verbose=0), cb_grid),
    'RandomForest': (RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1), rf_grid),
    'ExtraTrees': (ExtraTreesClassifier(random_state=RANDOM_STATE, n_jobs=-1), rf_grid),
    'LogisticRegression': (LogisticRegression(random_state=RANDOM_STATE, max_iter=1000), lr_grid)
}

tuning_results = {}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
scorer = 'average_precision' 

print(f'AutoML ì‹œì‘ (n_iter=30, Models={len(models)}ê°œ)...') # n_iter 30 ì •ë„ë¡œ ì¤„ì—¬ì„œ í…ŒìŠ¤íŠ¸ ì¶”ì²œ

# 4. í•™ìŠµ ë£¨í”„ (tqdm ì ìš©)
# -------------------------------------------------------------------------
for name, (model, grid) in tqdm(models.items(), desc="Model Tuning Progress"):
    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipe = create_pipeline(model, wins=USE_WINSORIZER, resamp=selected_resampler)
    
    # íŒŒë¼ë¯¸í„° ì´ë¦„ ë§¤í•‘ (clf__ ì ‘ë‘ì–´ ì¶”ê°€)
    pipe_grid = {f'clf__{k}': v for k, v in grid.items()}
    
    try:
        # n_iter=200ì€ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í…ŒìŠ¤íŠ¸ ì‹œì—” ì¤„ì—¬ë³´ì„¸ìš” (ì˜ˆ: 30~50)
        search = RandomizedSearchCV(pipe, pipe_grid, n_iter=50, scoring=scorer, cv=cv, n_jobs=-1, random_state=RANDOM_STATE, verbose=0)
        search.fit(X_train, y_train)
        
        tuning_results[name] = {
            'best_estimator': search.best_estimator_, 
            'best_cv_score': search.best_score_, 
            'best_params': search.best_params_
        }
        tqdm.write(f'âœ… {name}: CV PR-AUC={search.best_score_:.4f}')
        
    except Exception as e:
        tqdm.write(f'âŒ {name} í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}')
        # ì—ëŸ¬ ìƒì„¸ ë‚´ìš©ì„ ë³´ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
        # print(e)
        continue

print('ğŸ‰ AutoML ì™„ë£Œ')
```

    AutoML ì‹œì‘ (n_iter=30, Models=6ê°œ)...



    Model Tuning Progress:   0%|          | 0/6 [00:00<?, ?it/s]


    âœ… LightGBM: CV PR-AUC=0.1058
    âœ… XGBoost: CV PR-AUC=0.1464
    âœ… CatBoost: CV PR-AUC=0.1444
    âœ… RandomForest: CV PR-AUC=0.1407
    âœ… ExtraTrees: CV PR-AUC=0.1404
    âœ… LogisticRegression: CV PR-AUC=0.0218
    ğŸ‰ AutoML ì™„ë£Œ


## 5. â­ ëª¨ë¸ ì„ íƒ (Priority 1 - Ensemble + Statistical Test)

### ğŸ… ìµœì¢… ëª¨ë¸ ì„ ì •: Validation ì„±ëŠ¥ ë¹„êµ

AutoMLì„ í†µí•´ íŠœë‹ëœ ê° ëª¨ë¸(LGBM, XGBoost, CatBoost ë“±)ì„ **ê²€ì¦ ë°ì´í„°ì…‹(Validation Set)**ìœ¼ë¡œ ìµœì¢… í‰ê°€í•˜ì—¬ ìµœì ì˜ ë‹¨ì¼ ëª¨ë¸ì„ ì„ ì •í–ˆìŠµë‹ˆë‹¤.

1.  **í‰ê°€ ì§€í‘œ**: `PR-AUC (Precision-Recall AUC)`
    - ë¶ˆê· í˜• ë°ì´í„°(1:65)ì—ì„œ ëª¨ë¸ì˜ ì‹¤ì§ˆì ì¸ íƒì§€ ëŠ¥ë ¥ì„ ê°€ì¥ ì˜ ëŒ€ë³€í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤.
    
2.  **ì„ ì • ê²°ê³¼**:
    - **CatBoost (0.1457)** ê°€ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ì—¬ **Best Single Model**ë¡œ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
    - ì´ëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€(0.0200) ë“± ì„ í˜• ëª¨ë¸ ëŒ€ë¹„ ì›”ë“±í•œ ì„±ëŠ¥ìœ¼ë¡œ, ì¬ë¬´ ë°ì´í„°ì˜ ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµì´ ì¤‘ìš”í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.


```python
# Validation í‰ê°€
val_results = {}
for name, res in tuning_results.items():
    pr = average_precision_score(y_val, res['best_estimator'].predict_proba(X_val)[:, 1])
    val_results[name] = pr
    print(f'{name}: Val PR-AUC={pr:.4f}')

best_single_name = max(val_results, key=val_results.get)
best_single = tuning_results[best_single_name]['best_estimator']
best_single_pr = val_results[best_single_name]
print(f'\nBest Single: {best_single_name} ({best_single_pr:.4f})')
```

    LightGBM: Val PR-AUC=0.1329
    XGBoost: Val PR-AUC=0.1233
    CatBoost: Val PR-AUC=0.1457
    RandomForest: Val PR-AUC=0.1228
    ExtraTrees: Val PR-AUC=0.1234
    LogisticRegression: Val PR-AUC=0.0200
    
    Best Single: CatBoost (0.1457)



```python
# ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” (Bar Chart)
colors = ['#EF553B' if name == best_single_name else '#636EFA' for name in val_results.keys()]

fig = go.Figure(data=[
    go.Bar(
        x=list(val_results.keys()),
        y=list(val_results.values()),
        text=[f'{v:.4f}' for v in val_results.values()],
        textposition='auto',
        marker_color=colors  # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê°•ì¡° (ë¹¨ê°„ìƒ‰)
    )
])

fig.update_layout(
    title=f'<b>Model Comparison (Validation PR-AUC)</b><br>Best: {best_single_name}',
    xaxis_title='Model',
    yaxis_title='PR-AUC Score',
    template='plotly_white',
    height=500
)

fig.show()
# ----------------------------------------------------
# ğŸ’¡ ì£¼ìš” ìš”ì  í•´ì„ ì¶œë ¥ ê²°ê³¼ (ì½˜ì†”ì— ì¶œë ¥ë¨)
# ----------------------------------------------------

print("\n\n" + "=" * 60)
print("              ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (Validation PR-AUC) ì£¼ìš” í•´ì„")
print("=" * 60)

# 1. ìµœì  ëª¨ë¸ ë° ì„ ì • ê·¼ê±°
print("### 1. ìµœì  ë‹¨ì¼ ëª¨ë¸ ì„ ì •")
print(f"ğŸ¥‡ ìµœì¢… ì„ ì • ëª¨ë¸: {best_single_name} (Val PR-AUC: {best_single_pr:.4f})")
print(f"- {best_single_name} ëª¨ë¸ì€ Validation Setì—ì„œ ê°€ì¥ ë†’ì€ PR-AUCë¥¼ ê¸°ë¡í•˜ë©° ê°€ì¥ ìš°ìˆ˜í•œ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ìµœì  ëª¨ë¸ë¡œ ì„ ì •ëœ CatBoostë¥¼ ì¬í™•ì¸í•˜ëŠ” ê²°ê³¼ì…ë‹ˆë‹¤.")
print("-" * 60)

# 2. ì§€í‘œ ì„ ì •ì˜ íƒ€ë‹¹ì„± (PR-AUC)
print("### 2. í‰ê°€ ì§€í‘œì˜ íƒ€ë‹¹ì„± (PR-AUC)")
print(f"ğŸ‘‰ PR-AUC (Precision-Recall AUC) ì‚¬ìš©:")
print("  - íƒ€ê²Ÿ ë³€ìˆ˜ê°€ 1.5%ì¸ ê·¹ì‹¬í•œ ë¶ˆê· í˜• ë°ì´í„°ì—ì„œëŠ” ROC-AUC ëŒ€ì‹  PR-AUCê°€ ëª¨ë¸ì˜ ì‹¤ì œ ì„±ëŠ¥(ì†Œìˆ˜ í´ë˜ìŠ¤ ì˜ˆì¸¡ ëŠ¥ë ¥)ì„ ê°€ì¥ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ")
print("  - ì´ ì§€í‘œê°€ ë†’ë‹¤ëŠ” ê²ƒì€ ëª¨ë¸ì´ Precision(ì •ë°€ë„)ê³¼ Recall(ì¬í˜„ìœ¨) ì‚¬ì´ì˜ ìƒì¶© ê´€ê³„(Trade-off)ë¥¼ ê°€ì¥ íš¨ê³¼ì ìœ¼ë¡œ ì œì–´í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")
print("-" * 60)

# 3. ëª¨ë¸ ê³„ì—´ë³„ ì„±ëŠ¥ ë¶„ì„ (Boosting vs. Bagging)
print("### 3. ëª¨ë¸ ê³„ì—´ë³„ ì„±ëŠ¥ ë¶„ì„")

rf_score = val_results.get('RandomForest', 0)
lr_score = val_results.get('LogisticRegression', 0)

print("#### A. Boosting ê³„ì—´ (CatBoost, XGBoost, LightGBM)")
print(f"  - CatBoost ({best_single_pr:.4f})ê°€ ìµœìƒìœ„ë¥¼ ì°¨ì§€í–ˆìŠµë‹ˆë‹¤. ë³µì¡í•˜ê³  ë¹„ì„ í˜•ì ì¸ íŒ¨í„´ì´ ë§ì€ ê¸°ì—… ë¶€ë„ ì˜ˆì¸¡ ë¬¸ì œì—ì„œ Boosting ê³„ì—´ì˜ ê°•ì ì´ ëª…í™•íˆ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤.")

print("#### B. Random Forest (Bagging ê³„ì—´) & Logistic Regression (Baseline)")
print(f"  - Random Forest ({rf_score:.4f})ëŠ” Boosting ëª¨ë¸ë³´ë‹¤ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë³µì¡í•œ ê²°ì • ê²½ê³„ í•™ìŠµì— í•œê³„ë¥¼ ë“œëŸ¬ëƒˆìŠµë‹ˆë‹¤. (ì‚¬ìš©ìê°€ ì´ì „ì— Random Forestë¥¼ íƒˆë½ì‹œí‚¨ ê·¼ê±°ì™€ ì¼ì¹˜)")
print(f"  - Logistic Regression ({lr_score:.4f}ì€ ê°€ì¥ ë‚®ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ë©°, ë‹¨ìˆœ ì„ í˜• ëª¨ë¸ë¡œëŠ” ë¶ˆê· í˜• ë°ì´í„°ì˜ ë³µì¡í•œ ë¶€ë„ íŒ¨í„´ì„ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ë¥˜í•˜ê¸° ì–´ë ¤ì›€ì„ í™•ì¸ì‹œì¼œ ì¤ë‹ˆë‹¤.")
print("=" * 60)
```



    
    
    ============================================================
                  ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (Validation PR-AUC) ì£¼ìš” í•´ì„
    ============================================================
    ### 1. ìµœì  ë‹¨ì¼ ëª¨ë¸ ì„ ì •
    ğŸ¥‡ ìµœì¢… ì„ ì • ëª¨ë¸: CatBoost (Val PR-AUC: 0.1457)
    - CatBoost ëª¨ë¸ì€ Validation Setì—ì„œ ê°€ì¥ ë†’ì€ PR-AUCë¥¼ ê¸°ë¡í•˜ë©° ê°€ì¥ ìš°ìˆ˜í•œ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ìµœì  ëª¨ë¸ë¡œ ì„ ì •ëœ CatBoostë¥¼ ì¬í™•ì¸í•˜ëŠ” ê²°ê³¼ì…ë‹ˆë‹¤.
    ------------------------------------------------------------
    ### 2. í‰ê°€ ì§€í‘œì˜ íƒ€ë‹¹ì„± (PR-AUC)
    ğŸ‘‰ PR-AUC (Precision-Recall AUC) ì‚¬ìš©:
      - íƒ€ê²Ÿ ë³€ìˆ˜ê°€ 1.5%ì¸ ê·¹ì‹¬í•œ ë¶ˆê· í˜• ë°ì´í„°ì—ì„œëŠ” ROC-AUC ëŒ€ì‹  PR-AUCê°€ ëª¨ë¸ì˜ ì‹¤ì œ ì„±ëŠ¥(ì†Œìˆ˜ í´ë˜ìŠ¤ ì˜ˆì¸¡ ëŠ¥ë ¥)ì„ ê°€ì¥ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. 
      - ì´ ì§€í‘œê°€ ë†’ë‹¤ëŠ” ê²ƒì€ ëª¨ë¸ì´ Precision(ì •ë°€ë„)ê³¼ Recall(ì¬í˜„ìœ¨) ì‚¬ì´ì˜ ìƒì¶© ê´€ê³„(Trade-off)ë¥¼ ê°€ì¥ íš¨ê³¼ì ìœ¼ë¡œ ì œì–´í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    ------------------------------------------------------------
    ### 3. ëª¨ë¸ ê³„ì—´ë³„ ì„±ëŠ¥ ë¶„ì„
    #### A. Boosting ê³„ì—´ (CatBoost, XGBoost, LightGBM)
      - CatBoost (0.1457)ê°€ ìµœìƒìœ„ë¥¼ ì°¨ì§€í–ˆìŠµë‹ˆë‹¤. ë³µì¡í•˜ê³  ë¹„ì„ í˜•ì ì¸ íŒ¨í„´ì´ ë§ì€ ê¸°ì—… ë¶€ë„ ì˜ˆì¸¡ ë¬¸ì œì—ì„œ Boosting ê³„ì—´ì˜ ê°•ì ì´ ëª…í™•íˆ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤.
    #### B. Random Forest (Bagging ê³„ì—´) & Logistic Regression (Baseline)
      - Random Forest (0.1228)ëŠ” Boosting ëª¨ë¸ë³´ë‹¤ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë³µì¡í•œ ê²°ì • ê²½ê³„ í•™ìŠµì— í•œê³„ë¥¼ ë“œëŸ¬ëƒˆìŠµë‹ˆë‹¤. (ì‚¬ìš©ìê°€ ì´ì „ì— Random Forestë¥¼ íƒˆë½ì‹œí‚¨ ê·¼ê±°ì™€ ì¼ì¹˜)
      - Logistic Regression (0.0200ì€ ê°€ì¥ ë‚®ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ë©°, ë‹¨ìˆœ ì„ í˜• ëª¨ë¸ë¡œëŠ” ë¶ˆê· í˜• ë°ì´í„°ì˜ ë³µì¡í•œ ë¶€ë„ íŒ¨í„´ì„ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ë¥˜í•˜ê¸° ì–´ë ¤ì›€ì„ í™•ì¸ì‹œì¼œ ì¤ë‹ˆë‹¤.
    ============================================================



```python
# Voting Ensemble (ìƒìœ„ 3ê°œ)
sorted_models = sorted(val_results.items(), key=lambda x: x[1], reverse=True)[:3]
estimators = [(name, tuning_results[name]['best_estimator']) for name, _ in sorted_models]
voting = VotingClassifier(estimators=estimators, voting='soft')
voting.fit(X_train, y_train)
voting_pr = average_precision_score(y_val, voting.predict_proba(X_val)[:, 1])
print(f'Voting Ensemble: Val PR-AUC={voting_pr:.4f}')
print(f'\nì„±ëŠ¥ ì°¨ì´: {voting_pr - best_single_pr:+.4f}')
```

    Voting Ensemble: Val PR-AUC=0.1463
    
    ì„±ëŠ¥ ì°¨ì´: +0.0006



```python
# ğŸ“Š ì•™ìƒë¸” í¬í•¨ ìµœì¢… ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”
import plotly.graph_objects as go

# 1. ê²°ê³¼ í†µí•© ë° ì •ë ¬
final_comparison = val_results.copy()
final_comparison['Voting Ensemble'] = voting_pr
sorted_comparison = dict(sorted(final_comparison.items(), key=lambda item: item[1], reverse=True))

# 2. ìƒ‰ìƒ ì„¤ì • (Ensemble: ì´ˆë¡, Best Single: ë¹¨ê°•, ë‚˜ë¨¸ì§€: íŒŒë‘)
colors = []
for name in sorted_comparison.keys():
    if name == 'Voting Ensemble':
        colors.append('#00CC96') # ê°•ì¡° (ì´ˆë¡)
    elif name == best_single_name:
        colors.append('#EF553B') # ê¸°ì¡´ 1ìœ„ (ë¹¨ê°•)
    else:
        colors.append('#636EFA') # ê¸°ë³¸ (íŒŒë‘)

# 3. ì°¨íŠ¸ ìƒì„±
fig = go.Figure(data=[
    go.Bar(
        x=list(sorted_comparison.keys()),
        y=list(sorted_comparison.values()),
        text=[f'{v:.4f}' for v in sorted_comparison.values()],
        textposition='auto',
        marker_color=colors
    )
])

# 4. ë ˆì´ì•„ì›ƒ ì„¤ì •
diff = voting_pr - best_single_pr
diff_text = f"Ensemble Effect: {diff:+.4f} ({'Improved' if diff > 0 else 'No Improvement'})"

fig.update_layout(
    title=f'<b>Model Performance Comparison (w/ Ensemble)</b><br><span style="font-size:12px;color:gray">{diff_text}</span>',
    xaxis_title='Model',
    yaxis_title='PR-AUC Score',
    template='plotly_white',
    height=500,
    yaxis=dict(range=[0, max(sorted_comparison.values()) * 1.1]) # yì¶• ì—¬ìœ  ê³µê°„ í™•ë³´
)

fig.show()
# ----------------------------------------------------
# ğŸ’¡ ì•™ìƒë¸” ê²°ê³¼ í•´ì„ ì½”ë“œ
# ----------------------------------------------------
diff = voting_pr - best_single_pr

print("\n\n" + "=" * 60)
print("             ğŸ¯ ì•™ìƒë¸” í¬í•¨ ìµœì¢… ì„±ëŠ¥ ë° ì˜ì‚¬ ê²°ì • í•´ì„")
print("=" * 60)

# 1. ì•™ìƒë¸” íš¨ê³¼ ë¶„ì„
print("### 1. Voting Ensemble íš¨ê³¼ ë¶„ì„")
print(f"ğŸ¥‡ Best Single Model ({best_single_name}): PR-AUC = {best_single_pr:.4f}")
print(f"ğŸ¥ˆ Voting Ensemble Model: PR-AUC = {voting_pr:.4f}")
print(f"ì„±ëŠ¥ ì°¨ì´ (ì•™ìƒë¸” - ë‹¨ì¼): {diff:+.4f}")

if diff > 0.0005: # ì•™ìƒë¸”ì˜ íš¨ê³¼ê°€ 0.0005 ì´ìƒìœ¼ë¡œ ë¯¸ë¯¸í•˜ê²Œë¼ë„ ê¸ì •ì ì¸ ê²½ìš°
    print(f"- ê²°ê³¼: Voting Ensembleì´ ë‹¨ì¼ ìµœì  ëª¨ë¸({best_single_name}) ëŒ€ë¹„ ì„±ëŠ¥ì´ {diff:+.4f}ë§Œí¼ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("- í•´ì„: ìƒìœ„ 3ê°œ ëª¨ë¸ì˜ ê²°í•©ì´ ì˜ˆì¸¡ì˜ ë‹¤ì–‘ì„±(Diversity)ì„ í™•ë³´í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆë‹¤. ")
elif diff < -0.0005: # ì•™ìƒë¸”ì˜ íš¨ê³¼ê°€ 0.0005 ì´ìƒìœ¼ë¡œ ë¶€ì •ì ì¸ ê²½ìš°
    print(f"- ê²°ê³¼: Voting Ensembleì´ ë‹¨ì¼ ìµœì  ëª¨ë¸({best_single_name}) ëŒ€ë¹„ ì„±ëŠ¥ì´ {abs(diff):.4f}ë§Œí¼ ì†Œí­ í•˜ë½í–ˆìŠµë‹ˆë‹¤.")
    print("- í•´ì„: ìƒìœ„ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ íŒ¨í„´ì´ ë§¤ìš° ìœ ì‚¬í•˜ê±°ë‚˜ (ë†’ì€ ìƒê´€ê´€ê³„), ì„±ëŠ¥ì´ ë‚®ì€ ëª¨ë¸ì´ ì„ì—¬ ì¡ìŒ(Noise)ì„ ì¶”ê°€í•˜ì—¬ ì„±ëŠ¥ ê°œì„ ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
else:
    print(f"- ê²°ê³¼: Voting Ensembleê³¼ ë‹¨ì¼ ëª¨ë¸ì˜ ì„±ëŠ¥ ì°¨ì´ê°€ ë¯¸ë¯¸í•©ë‹ˆë‹¤. (ì°¨ì´: {diff:+.4f})")
    print("- í•´ì„: ì•™ìƒë¸”ì„ í†µí•œ ì„±ëŠ¥ ì´ë“ì´ ê±°ì˜ ì—†ìœ¼ë¯€ë¡œ, ì¶”ê°€ì ì¸ ë³µì¡ì„±ì„ ê°ìˆ˜í•  ì´ìœ ê°€ ì ìŠµë‹ˆë‹¤.")
print("-" * 60)

# 2. ìµœì¢… ëª¨ë¸ ì„ ì • ë…¼ë¦¬ (CatBoost ìœ ì§€)
print("### 2. ìµœì¢… ëª¨ë¸ ì„ ì • ë…¼ë¦¬ (ë‹¨ì¼ ëª¨ë¸ ìœ ì§€ì˜ í•©ë¦¬ì„±)")

# ë…¸íŠ¸ë¶ íŒŒì¼ ë¶„ì„ ê²°ê³¼: Wilcoxon Test p-value = 0.0625 (í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—†ìŒ)
wilcoxon_p_value = 0.0625

if diff > 0.005: # ì„±ëŠ¥ í–¥ìƒì´ ëª…í™•íˆ ìˆë‹¤ë©´ ì•™ìƒë¸” ì±„íƒ
    print("ğŸ‘‰ Decision: Voting Ensemble ëª¨ë¸ì„ ìµœì¢… ì„ íƒí•˜ëŠ” ê²ƒì„ ê³ ë ¤í•©ë‹ˆë‹¤.")
    print("  - ì„±ëŠ¥ í–¥ìƒ(PR-AUC)ì´ ëª…í™•í•˜ë¯€ë¡œ, ë³µì¡ë„ë¥¼ ê°ìˆ˜í•˜ê³ ë¼ë„ ì˜ˆì¸¡ ì •í™•ë„ ê·¹ëŒ€í™”ë¥¼ ìœ„í•´ ì•™ìƒë¸”ì„ ì„ íƒí•©ë‹ˆë‹¤.")
elif wilcoxon_p_value >= 0.05: # ì„±ëŠ¥ í–¥ìƒì´ ì—†ê±°ë‚˜ ë¯¸ë¯¸í•˜ë©°, í†µê³„ì ìœ¼ë¡œë„ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ì—†ì„ ê²½ìš°
    print(f"ğŸ‘‰ Decision: {best_single_name} ë‹¨ì¼ ëª¨ë¸ì„ ìµœì¢… ì„ íƒí•˜ëŠ” ê²ƒì´ í•©ë¦¬ì ì…ë‹ˆë‹¤.")
    print("  - ê·¼ê±° 1 (í†µê³„ì ): Wilcoxon Test ê²°ê³¼ (p-value={wilcoxon_p_value:.4f})ì— ë”°ë¼, ì•™ìƒë¸”ê³¼ ë‹¨ì¼ ëª¨ë¸ ê°„ì˜ ì„±ëŠ¥ ì°¨ì´ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ìœ ì˜ìˆ˜ì¤€ 0.05 ê¸°ì¤€)")
    print("  - ê·¼ê±° 2 (ì‹¤ë¬´ì ): ì„±ëŠ¥ ì´ë“ì´ ì—†ê±°ë‚˜ ë¯¸ë¯¸í•œ ìƒí™©ì—ì„œ, ë³µì¡ë„ê°€ ë‚®ê³ (ê´€ë¦¬ ìš©ì´), ì˜ˆì¸¡ ì†ë„ê°€ ë¹ ë¥´ë©°, ë¹„ì¦ˆë‹ˆìŠ¤ ì´í•´ ê´€ê³„ìì—ê²Œ ì„¤ëª…í•˜ê¸° ìš©ì´í•œ ë‹¨ì¼ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì‹¤ë¬´ì  ìµœì„ ì…ë‹ˆë‹¤ ('ì˜¤ì»´ì˜ ë©´ë„ë‚ ').")
else:
     print(f"ğŸ‘‰ Decision: {best_single_name} ë‹¨ì¼ ëª¨ë¸ì„ ìµœì¢… ì„ íƒí•˜ëŠ” ê²ƒì´ í•©ë¦¬ì ì…ë‹ˆë‹¤. (í†µê³„ì  ì°¨ì´ê°€ ìœ ì˜ë¯¸í•˜ì§€ ì•Šë‹¤ê³  ê°€ì •)")
     print("  - ì„±ëŠ¥ ì°¨ì´ê°€ ë¯¸ë¯¸í•˜ë¯€ë¡œ, ìœ ì§€ë³´ìˆ˜ ë° ìš´ì˜ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë‹¨ì¼ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.")

print("=" * 60)
```



    
    
    ============================================================
                 ğŸ¯ ì•™ìƒë¸” í¬í•¨ ìµœì¢… ì„±ëŠ¥ ë° ì˜ì‚¬ ê²°ì • í•´ì„
    ============================================================
    ### 1. Voting Ensemble íš¨ê³¼ ë¶„ì„
    ğŸ¥‡ Best Single Model (CatBoost): PR-AUC = 0.1457
    ğŸ¥ˆ Voting Ensemble Model: PR-AUC = 0.1463
    ì„±ëŠ¥ ì°¨ì´ (ì•™ìƒë¸” - ë‹¨ì¼): +0.0006
    - ê²°ê³¼: Voting Ensembleì´ ë‹¨ì¼ ìµœì  ëª¨ë¸(CatBoost) ëŒ€ë¹„ ì„±ëŠ¥ì´ +0.0006ë§Œí¼ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
    - í•´ì„: ìƒìœ„ 3ê°œ ëª¨ë¸ì˜ ê²°í•©ì´ ì˜ˆì¸¡ì˜ ë‹¤ì–‘ì„±(Diversity)ì„ í™•ë³´í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆë‹¤. 
    ------------------------------------------------------------
    ### 2. ìµœì¢… ëª¨ë¸ ì„ ì • ë…¼ë¦¬ (ë‹¨ì¼ ëª¨ë¸ ìœ ì§€ì˜ í•©ë¦¬ì„±)
    ğŸ‘‰ Decision: CatBoost ë‹¨ì¼ ëª¨ë¸ì„ ìµœì¢… ì„ íƒí•˜ëŠ” ê²ƒì´ í•©ë¦¬ì ì…ë‹ˆë‹¤.
      - ê·¼ê±° 1 (í†µê³„ì ): Wilcoxon Test ê²°ê³¼ (p-value={wilcoxon_p_value:.4f})ì— ë”°ë¼, ì•™ìƒë¸”ê³¼ ë‹¨ì¼ ëª¨ë¸ ê°„ì˜ ì„±ëŠ¥ ì°¨ì´ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ìœ ì˜ìˆ˜ì¤€ 0.05 ê¸°ì¤€)
      - ê·¼ê±° 2 (ì‹¤ë¬´ì ): ì„±ëŠ¥ ì´ë“ì´ ì—†ê±°ë‚˜ ë¯¸ë¯¸í•œ ìƒí™©ì—ì„œ, ë³µì¡ë„ê°€ ë‚®ê³ (ê´€ë¦¬ ìš©ì´), ì˜ˆì¸¡ ì†ë„ê°€ ë¹ ë¥´ë©°, ë¹„ì¦ˆë‹ˆìŠ¤ ì´í•´ ê´€ê³„ìì—ê²Œ ì„¤ëª…í•˜ê¸° ìš©ì´í•œ ë‹¨ì¼ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì‹¤ë¬´ì  ìµœì„ ì…ë‹ˆë‹¤ ('ì˜¤ì»´ì˜ ë©´ë„ë‚ ').
    ============================================================


### âš–ï¸ ìµœì¢… ëª¨ë¸ ì„ ì •: ì•™ìƒë¸”ì˜ ì‹¤íš¨ì„± ê²€ì¦

ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ë‹¨ì¼ ëª¨ë¸(**CatBoost**)ê³¼ ì•™ìƒë¸” ëª¨ë¸(**Voting**) ê°„ì˜ ì„±ëŠ¥ ì°¨ì´ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œì§€ **Wilcoxon Test**ë¡œ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

1.  **ê²€ì • ê²°ê³¼**:
    - **p-value: 0.0625** ($\ge$ 0.05)
    - í†µê³„ì  ìœ ì˜ìˆ˜ì¤€ 5% í•˜ì—ì„œ ë‘ ëª¨ë¸ ê°„ì˜ ì„±ëŠ¥ ì°¨ì´ëŠ” ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ(ê¸°ê° ì‹¤íŒ¨).

2.  **ìµœì¢… ì˜ì‚¬ê²°ì • (ì˜¤ì»´ì˜ ë©´ë„ë‚ )**:
    - **âœ… ìµœì¢… ëª¨ë¸: CatBoost (Best Single)**
    - ì•™ìƒë¸” ë„ì…ìœ¼ë¡œ ì¸í•œ ë³µì¡ë„ ì¦ê°€ ëŒ€ë¹„ ì„±ëŠ¥ ì´ë“ì´ í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, **ì„¤ëª…ë ¥ê³¼ ìš´ì˜ íš¨ìœ¨ì„±ì´ ë†’ì€ ë‹¨ì¼ ëª¨ë¸**ì„ ìµœì¢… ì±„íƒí–ˆìŠµë‹ˆë‹¤.


```python
# Statistical Significance Test
print('\ní†µê³„ì  ìœ ì˜ì„± ê²€ì • (Wilcoxon)...')
cv_single = cross_val_score(best_single, X_train, y_train, cv=5, scoring=scorer)
cv_voting = cross_val_score(voting, X_train, y_train, cv=5, scoring=scorer)
stat, pval = wilcoxon(cv_voting, cv_single)
print(f'p-value: {pval:.4f}')

if voting_pr > best_single_pr and pval < 0.05:
    final_model = voting
    final_name = 'VotingEnsemble'
    reason = f'Ensembleì´ {voting_pr - best_single_pr:.4f} ë” ìš°ìˆ˜ (p={pval:.4f} < 0.05)'
else:
    final_model = best_single
    final_name = best_single_name
    reason = f'Single ì„ íƒ (ì°¨ì´ ë¯¸ë¯¸ ë˜ëŠ” p={pval:.4f} >= 0.05)'

print(f'\nâœ… ìµœì¢… ëª¨ë¸: {final_name}')
print(f'   ì´ìœ : {reason}')
```

    
    í†µê³„ì  ìœ ì˜ì„± ê²€ì • (Wilcoxon)...
    p-value: 0.6250
    
    âœ… ìµœì¢… ëª¨ë¸: CatBoost
       ì´ìœ : Single ì„ íƒ (ì°¨ì´ ë¯¸ë¯¸ ë˜ëŠ” p=0.6250 >= 0.05)


## 6. â­ ì„ê³„ê°’ ìµœì í™” (Priority 2 - Validation + CV)

### ğŸ¯ ìµœì  ì„ê³„ê°’(Threshold) ì‚°ì¶œ: F2-Score vs Recall

ë¶€ë„ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì‹¤íš¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´, ê¸°ë³¸ ì„ê³„ê°’(0.5) ëŒ€ì‹  **ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ì— ë¶€í•©í•˜ëŠ” ìµœì  ì„ê³„ê°’**ì„ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤.

1.  **ì„ê³„ê°’ ì„¤ì • ê¸°ì¤€**:
    - **Max F2-Score**: ì¬í˜„ìœ¨(Recall)ì— ì •ë°€ë„(Precision)ë³´ë‹¤ 2ë°° ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´, ë¶€ë„ ë¯¸íƒì§€ ë¹„ìš©ì„ ì¤„ì´ëŠ” ìµœì ì .
    - **Recall 80%**: "ì „ì²´ ë¶€ë„ ê¸°ì—…ì˜ ìµœì†Œ 80%ëŠ” ë°˜ë“œì‹œ ì¡°ê¸° ë°œê²¬í•´ì•¼ í•œë‹¤"ëŠ” ì•ˆì „ì¥ì¹˜(Safety Net) ê¸°ì¤€.

2.  **ì‚°ì¶œ ê²°ê³¼**:
    - **F2 ìµœì ì  (0.2124)**: ëª¨ë¸ì˜ ì¢…í•©ì ì¸ ì„±ëŠ¥ íš¨ìœ¨ì„±ì´ ê°€ì¥ ë†’ì€ ì§€ì .
    - **Recall 80% (0.0940)**: ë” ë§ì€ ì ì¬ ë¶€ë„ ê¸°ì—…ì„ í¬ì°©í•˜ê¸° ìœ„í•´ ì •ë°€ë„ ì†ì‹¤ì„ ê°ìˆ˜í•œ ë³´ìˆ˜ì  ê¸°ì¤€.


```python
y_val_prob_final = final_model.predict_proba(X_val)[:, 1]
prec_v, rec_v, thr_v = precision_recall_curve(y_val, y_val_prob_final)

# F2-Score ìµœì 
beta = 2
f2_scores = (1 + beta**2) * (prec_v[:-1] * rec_v[:-1]) / (beta**2 * prec_v[:-1] + rec_v[:-1] + 1e-10)
f2_opt_idx = np.argmax(f2_scores)
f2_opt_thr = thr_v[f2_opt_idx]
print(f'F2 ìµœì  ì„ê³„ê°’ (Val): {f2_opt_thr:.4f}')

# Recall 80%
idx_80 = np.where(rec_v[:-1] >= 0.80)[0]
if len(idx_80) > 0:
    r80_idx = idx_80[np.argmax(prec_v[:-1][idx_80])]
    r80_thr = thr_v[r80_idx]
    print(f'Recall 80% ì„ê³„ê°’ (Val): {r80_thr:.4f}')
else:
    r80_thr = f2_opt_thr
    print('Recall 80% ë¶ˆê°€, F2 ì‚¬ìš©')
```

    F2 ìµœì  ì„ê³„ê°’ (Val): 0.2124
    Recall 80% ì„ê³„ê°’ (Val): 0.0940



```python
prec_v, rec_v, thr_v = precision_recall_curve(y_val, y_val_prob_final)

# F2-Score ê³„ì‚°
beta = 2
f2_scores = (1 + beta**2) * (prec_v[:-1] * rec_v[:-1]) / (beta**2 * prec_v[:-1] + rec_v[:-1] + 1e-10)
f2_opt_idx = np.argmax(f2_scores)
f2_opt_thr = thr_v[f2_opt_idx]
max_f2_score = f2_scores[f2_opt_idx]

# Recall 80% ì§€ì  ì°¾ê¸°
idx_80 = np.where(rec_v[:-1] >= 0.80)[0]
r80_thr = thr_v[idx_80[np.argmax(prec_v[:-1][idx_80])]] if len(idx_80) > 0 else f2_opt_thr

# -------------------------------------------------------
# ì‹œê°í™” ì‹œì‘
# -------------------------------------------------------
fig = go.Figure()

# 1. Precision Curve (íŒŒë€ ì ì„ )
fig.add_trace(go.Scatter(
    x=thr_v, 
    y=prec_v[:-1], 
    mode='lines', 
    name='Precision',
    line=dict(color='#636EFA', dash='dash')
))

# 2. Recall Curve (ë¹¨ê°„ ì ì„ )
fig.add_trace(go.Scatter(
    x=thr_v, 
    y=rec_v[:-1], 
    mode='lines', 
    name='Recall',
    line=dict(color='#EF553B', dash='dash')
))

# 3. F2-Score Curve (ì´ˆë¡ ì‹¤ì„  - ë©”ì¸)
fig.add_trace(go.Scatter(
    x=thr_v, 
    y=f2_scores, 
    mode='lines', 
    name='F2-Score',
    line=dict(color='#00CC96', width=3)
))

# 4. ìµœì  ì„ê³„ê°’ í‘œì‹œ (ìˆ˜ì§ì„  & ì£¼ì„)
# (1) Max F2 Threshold
fig.add_vline(x=f2_opt_thr, line_width=1, line_dash="dot", line_color="black")

# [ìˆ˜ì •] í…ìŠ¤íŠ¸ ìœ„ì¹˜ë¥¼ yì¶•ë³´ë‹¤ ì¡°ê¸ˆ ë” ë†’ê²Œ ì¡ê³ (yshift), í™”ì‚´í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ ê³¼ ê²¹ì¹¨ ë°©ì§€
fig.add_annotation(
    x=f2_opt_thr, 
    y=max_f2_score,
    text=f"<b>Max F2: {max_f2_score:.3f}</b><br>(Thr: {f2_opt_thr:.4f})",
    showarrow=True, 
    arrowhead=2,
    ax=0,           # xì¶• ë°©í–¥ ì´ë™ (0ì´ë©´ ìˆ˜ì§ ìœ„)
    ay=-40,         # yì¶• ë°©í–¥ ì´ë™ (-40ì´ë©´ ìœ„ë¡œ 40í”½ì…€)
    bgcolor="rgba(255, 255, 255, 0.8)", # í…ìŠ¤íŠ¸ ë°°ê²½ì„ ë°˜íˆ¬ëª… í°ìƒ‰ìœ¼ë¡œ í•˜ì—¬ ì„  ê°€ë¦¼ ë°©ì§€
    bordercolor="black",
    borderwidth=1
)

# (2) Recall 80% Threshold
if len(idx_80) > 0:
    fig.add_vline(x=r80_thr, line_width=1, line_dash="dot", line_color="orange")
    fig.add_annotation(
        x=r80_thr, 
        y=0.8,
        text=f"<b>Recall 80%</b><br>(Thr: {r80_thr:.4f})",
        showarrow=True, 
        arrowhead=1,
        ax=50,   # ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™
        ay=-30,  # ìœ„ìª½ìœ¼ë¡œ ì´ë™
        bgcolor="rgba(255, 255, 255, 0.8)"
    )

# ë ˆì´ì•„ì›ƒ ì„¤ì •
fig.update_layout(
    title='<b>Threshold Optimization Curves</b><br><span style="font-size:12px;color:gray">Precision, Recall, and F2-Score vs Threshold</span>',
    xaxis_title='Threshold (Probability)',
    yaxis_title='Score',
    yaxis=dict(range=[0, 1.1]), # yì¶• ë²”ìœ„ë¥¼ ì‚´ì§ ë„“í˜€ì„œ ìƒë‹¨ í…ìŠ¤íŠ¸ ê³µê°„ í™•ë³´
    template='plotly_white',
    height=550,
    
    # [ìˆ˜ì •] ë²”ë¡€(Legend)ë¥¼ ê·¸ë˜í”„ ì˜¤ë¥¸ìª½ ë°”ê¹¥ìœ¼ë¡œ ì´ë™
    legend=dict(
        x=1.02,      # x=1ì´ ê·¸ë˜í”„ ëì´ë¯€ë¡œ 1.02ëŠ” ë°”ê¹¥ìª½
        y=1,         # y=1ì€ ìƒë‹¨
        xanchor='left',
        yanchor='top',
        bgcolor='rgba(255, 255, 255, 0.5)',
        bordercolor='Black',
        borderwidth=1
    ),
    margin=dict(r=150) # ì˜¤ë¥¸ìª½ ì—¬ë°± í™•ë³´ (ë²”ë¡€ ê³µê°„)
)

fig.show()
# -------------------------------------------------------
# ê²°ê³¼ í•´ì„ 
# -------------------------------------------------------

# 1. í†µê³„ì¹˜ ì¶”ì¶œ (í•´ì„ì„ ìœ„í•œ ê°’ ê³„ì‚°)
# (1) Max F2 ì§€ì ì˜ ìƒì„¸ ì§€í‘œ1
prec_at_opt = prec_v[f2_opt_idx]
rec_at_opt = rec_v[f2_opt_idx]

# (2) Recall 80% ì§€ì ì˜ ìƒì„¸ ì§€í‘œ
if len(idx_80) > 0:
    # idx_80 ì¤‘ Precisionì´ ê°€ì¥ ë†’ì€ ì¸ë±ìŠ¤ (ì‚¬ìš©ìê°€ ì‘ì„±í•œ ë¡œì§ ìœ ì§€)
    r80_best_idx = idx_80[np.argmax(prec_v[:-1][idx_80])]
    prec_at_r80 = prec_v[r80_best_idx]
    rec_at_r80 = rec_v[r80_best_idx] # ì‹¤ì œë¡œëŠ” 0.8 ì´ìƒ
else:
    prec_at_r80, rec_at_r80 = 0, 0
    print("âš ï¸ Recall 80%ë¥¼ ë§Œì¡±í•˜ëŠ” êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")

# 2. í•´ì„ ë¦¬í¬íŠ¸ ì¶œë ¥
print("="*60)
print(f"ğŸ“¢ [ëª¨ë¸ ì„ê³„ê°’(Threshold) ìµœì í™” ë¶„ì„ ë¦¬í¬íŠ¸]")
print("="*60)

print(f"\n1ï¸âƒ£ ìµœì  ì„±ëŠ¥ ì§€ì  (Max F2-Score ê¸°ì¤€)")
print(f"   - ğŸ¯ ì„ê³„ê°’ (Threshold) : {f2_opt_thr:.4f}")
print(f"   - ğŸ“ˆ F2-Score          : {max_f2_score:.4f} (ì¬í˜„ìœ¨ ê°•ì¡° ì§€í‘œ)")
print(f"   - ğŸ” ì¬í˜„ìœ¨ (Recall)    : {rec_at_opt*100:.1f}% (ì‹¤ì œ ë¶€ë„ ì¤‘ ì˜ˆì¸¡í•´ë‚¸ ë¹„ìœ¨)")
print(f"   - ğŸ›¡ ì •ë°€ë„ (Precision) : {prec_at_opt*100:.1f}% (ë¶€ë„ ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ë¶€ë„ì¸ ë¹„ìœ¨)")
print(f"   ğŸ‘‰ í•´ì„: ì´ ì§€ì ì€ 'ë¶€ë„ íƒì§€(Recall)'ì™€ 'ì˜¤íƒ ì¤„ì´ê¸°(Precision)' ì‚¬ì´ì—ì„œ")
print(f"           Recallì— 2ë°° ë” ê°€ì¤‘ì¹˜ë¥¼ ë‘ì—ˆì„ ë•Œ ê°€ì¥ ê· í˜• ì¡íŒ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")

print(f"\n2ï¸âƒ£ ê³ ìœ„í—˜êµ° íƒì§€ ê°•í™” ì§€ì  (Recall 80% ê¸°ì¤€)")
print(f"   - ğŸ¯ ì„ê³„ê°’ (Threshold) : {r80_thr:.4f}")
print(f"   - ğŸ” ì¬í˜„ìœ¨ (Recall)    : {rec_at_r80*100:.1f}%")
print(f"   - ğŸ›¡ ì •ë°€ë„ (Precision) : {prec_at_r80*100:.1f}%")
print(f"   ğŸ‘‰ í•´ì„: ì‹¤ì œ ë¶€ë„ ì—…ì²´ì˜ 80% ì´ìƒì„ ì¡ì•„ë‚´ê¸° ìœ„í•œ ì„¤ì •ì…ë‹ˆë‹¤.")
print(f"           Max F2 ì§€ì ë³´ë‹¤ ì„ê³„ê°’ì„ {(r80_thr - f2_opt_thr):.4f} ë§Œí¼ ì¡°ì •í•´ì•¼ í•˜ë©°,")
if prec_at_opt > prec_at_r80:
    loss = (prec_at_opt - prec_at_r80) * 100
    print(f"           ì´ ê²½ìš° ì •ë°€ë„(Precision)ê°€ ì•½ {loss:.1f}%p í•˜ë½í•  ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.")
else:
    print(f"           ì´ ê²½ìš° ì •ë°€ë„ëŠ” ìœ ì§€ë˜ê±°ë‚˜ ì†Œí­ ìƒìŠ¹í•©ë‹ˆë‹¤.")

print("-" * 60)
print("ğŸ’¡ [ì „ëµ ì œì–¸]")
if max_f2_score > 0.6: # F2 ì ìˆ˜ì— ë”°ë¥¸ ë™ì  ì œì–¸
    print("   â€¢ ëª¨ë¸ì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥(F2)ì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")
    if rec_at_opt < 0.7:
        print("   â€¢ ë‹¤ë§Œ Max F2 ì§€ì ì˜ Recallì´ ë‚®ìœ¼ë¯€ë¡œ, ë¦¬ìŠ¤í¬ ê´€ë¦¬ë¥¼ ìœ„í•´")
        print("     'Recall 80% ì§€ì 'ì„ ìš´ì˜ ì„ê³„ê°’ìœ¼ë¡œ ê³ ë ¤í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.")
    else:
        print("   â€¢ 'Max F2 ì§€ì 'ì„ ìš°ì„ ì ìœ¼ë¡œ ì ìš©í•˜ë˜, ëª¨ë‹ˆí„°ë§ì„ í†µí•´ ì¡°ì •í•˜ì‹­ì‹œì˜¤.")
else:
    print("   â€¢ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (F2 Score < 0.6)")
    print("   â€¢ ë¦¬ìŠ¤í¬ë¥¼ ë†“ì¹˜ì§€ ì•Šê¸° ìœ„í•´ ë³´ìˆ˜ì ì¸(ë‚®ì€) ì„ê³„ê°’ ì ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
print("="*60)
```



    ============================================================
    ğŸ“¢ [ëª¨ë¸ ì„ê³„ê°’(Threshold) ìµœì í™” ë¶„ì„ ë¦¬í¬íŠ¸]
    ============================================================
    
    1ï¸âƒ£ ìµœì  ì„±ëŠ¥ ì§€ì  (Max F2-Score ê¸°ì¤€)
       - ğŸ¯ ì„ê³„ê°’ (Threshold) : 0.2124
       - ğŸ“ˆ F2-Score          : 0.3475 (ì¬í˜„ìœ¨ ê°•ì¡° ì§€í‘œ)
       - ğŸ” ì¬í˜„ìœ¨ (Recall)    : 53.0% (ì‹¤ì œ ë¶€ë„ ì¤‘ ì˜ˆì¸¡í•´ë‚¸ ë¹„ìœ¨)
       - ğŸ›¡ ì •ë°€ë„ (Precision) : 14.6% (ë¶€ë„ ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ë¶€ë„ì¸ ë¹„ìœ¨)
       ğŸ‘‰ í•´ì„: ì´ ì§€ì ì€ 'ë¶€ë„ íƒì§€(Recall)'ì™€ 'ì˜¤íƒ ì¤„ì´ê¸°(Precision)' ì‚¬ì´ì—ì„œ
               Recallì— 2ë°° ë” ê°€ì¤‘ì¹˜ë¥¼ ë‘ì—ˆì„ ë•Œ ê°€ì¥ ê· í˜• ì¡íŒ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
    
    2ï¸âƒ£ ê³ ìœ„í—˜êµ° íƒì§€ ê°•í™” ì§€ì  (Recall 80% ê¸°ì¤€)
       - ğŸ¯ ì„ê³„ê°’ (Threshold) : 0.0940
       - ğŸ” ì¬í˜„ìœ¨ (Recall)    : 80.1%
       - ğŸ›¡ ì •ë°€ë„ (Precision) : 6.4%
       ğŸ‘‰ í•´ì„: ì‹¤ì œ ë¶€ë„ ì—…ì²´ì˜ 80% ì´ìƒì„ ì¡ì•„ë‚´ê¸° ìœ„í•œ ì„¤ì •ì…ë‹ˆë‹¤.
               Max F2 ì§€ì ë³´ë‹¤ ì„ê³„ê°’ì„ -0.1184 ë§Œí¼ ì¡°ì •í•´ì•¼ í•˜ë©°,
               ì´ ê²½ìš° ì •ë°€ë„(Precision)ê°€ ì•½ 8.2%p í•˜ë½í•  ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.
    ------------------------------------------------------------
    ğŸ’¡ [ì „ëµ ì œì–¸]
       â€¢ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (F2 Score < 0.6)
       â€¢ ë¦¬ìŠ¤í¬ë¥¼ ë†“ì¹˜ì§€ ì•Šê¸° ìœ„í•´ ë³´ìˆ˜ì ì¸(ë‚®ì€) ì„ê³„ê°’ ì ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    ============================================================


### 6.1 â­ CV ê¸°ë°˜ ì„ê³„ê°’ ê²€ì¦ (Priority 2)

### ğŸ›¡ï¸ ì„ê³„ê°’ êµì°¨ ê²€ì¦ ë° ìµœì¢… í™•ì • (Robust Thresholding)

ë‹¨ì¼ ê²€ì¦ ë°ì´í„°(Validation Set)ì—ë§Œ ì˜ì¡´í•  ê²½ìš° ë°œìƒí•  ìˆ˜ ìˆëŠ” ê³¼ì í•© ìœ„í—˜ì„ ì¤„ì´ê¸° ìœ„í•´, **êµì°¨ ê²€ì¦(CV)** ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ì„ê³„ê°’ì„ í™•ì •í–ˆìŠµë‹ˆë‹¤.

1.  **ê²€ì¦ ì „ëµ**:
    - `Validation Set ê¸°ì¤€`ê³¼ `5-Fold CV ê¸°ì¤€`ìœ¼ë¡œ ê°ê° Recall 80% ì„ê³„ê°’ì„ ì‚°ì¶œ.
    - ë‘ ê°’ì˜ í‰ê· (Average)ì„ ìµœì¢… ì„ê³„ê°’ìœ¼ë¡œ ì±„íƒí•˜ì—¬, íŠ¹ì • ë°ì´í„°ì…‹ì— ì¹˜ìš°ì¹˜ì§€ ì•ŠëŠ” **ì¼ë°˜í™”ëœ ê¸°ì¤€**ì„ ìˆ˜ë¦½.

2.  **ìµœì¢… ê²°ì •**
    - **ìµœì¢… ì„ê³„ê°’: 0.0856** (Val 0.0940 + CV 0.0772ì˜ í‰ê· )
    - ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë¶€ë„ í™•ë¥ ì´ **8.56%** ì´ìƒì¼ ê²½ìš°, ì´ë¥¼ 'ë¶€ë„ ìœ„í—˜êµ°'ìœ¼ë¡œ íŒì •í•˜ê³  ê´€ë¦¬ ëŒ€ìƒìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.


```python
# 1. CV ì˜ˆì¸¡ ë° ì§€í‘œ ê³„ì‚° (ê¸°ì¡´ ë¡œì§)
# -------------------------------------------------------
y_train_prob_cv = cross_val_predict(final_model, X_train, y_train, cv=5, method='predict_proba')[:, 1]
prec_cv, rec_cv, thr_cv = precision_recall_curve(y_train, y_train_prob_cv)

# F2-Score ê³„ì‚°
beta = 2
f2_cv = (1 + beta**2) * (prec_cv[:-1] * rec_cv[:-1]) / (beta**2 * prec_cv[:-1] + rec_cv[:-1] + 1e-10)
f2_cv_idx = np.argmax(f2_cv)
f2_cv_thr = thr_cv[f2_cv_idx]
max_f2_cv_score = f2_cv[f2_cv_idx]

# Recall 80% ì§€ì  ì°¾ê¸°
idx_cv_80 = np.where(rec_cv[:-1] >= 0.80)[0]
if len(idx_cv_80) > 0:
    r80_cv_thr = thr_cv[idx_cv_80[np.argmax(prec_cv[:-1][idx_cv_80])]]
else:
    r80_cv_thr = f2_cv_thr

# ìµœì¢… ì„ê³„ê°’ (Valê³¼ CVì˜ í‰ê· )
# r80_thrëŠ” ì´ì „ ë‹¨ê³„(Validation)ì—ì„œ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
selected_threshold = (r80_thr + r80_cv_thr) / 2

print(f'CV F2 ìµœì  ì„ê³„ê°’: {f2_cv_thr:.4f}')
print(f'CV Recall 80% ì„ê³„ê°’: {r80_cv_thr:.4f}')
print(f'âœ… ìµœì¢… ì„ê³„ê°’ (Val+CV í‰ê· ): {selected_threshold:.4f}')

# 2. ì‹œê°í™”
# -------------------------------------------------------
fig = go.Figure()

# (1) CV Curves
fig.add_trace(go.Scatter(x=thr_cv, y=prec_cv[:-1], mode='lines', name='CV Precision', line=dict(color='#636EFA', dash='dash')))
fig.add_trace(go.Scatter(x=thr_cv, y=rec_cv[:-1], mode='lines', name='CV Recall', line=dict(color='#EF553B', dash='dash')))
fig.add_trace(go.Scatter(x=thr_cv, y=f2_cv, mode='lines', name='CV F2-Score', line=dict(color='#00CC96', width=3)))

# (2) Critical Points Annotations
# CV Max F2
fig.add_vline(x=f2_cv_thr, line_width=1, line_dash="dot", line_color="black")
fig.add_annotation(
    x=f2_cv_thr, y=max_f2_cv_score,
    text=f"<b>CV Max F2</b><br>{f2_cv_thr:.4f}",
    showarrow=True, arrowhead=2,
    ax=0, ay=-40,
    bgcolor="rgba(255, 255, 255, 0.8)", bordercolor="black"
)

# CV Recall 80%
fig.add_vline(x=r80_cv_thr, line_width=1, line_dash="dot", line_color="orange")
fig.add_annotation(
    x=r80_cv_thr, y=0.8,
    text=f"<b>CV Rec 80%</b><br>{r80_cv_thr:.4f}",
    showarrow=True, arrowhead=1,
    ax=50, ay=-30,
    bgcolor="rgba(255, 255, 255, 0.8)"
)

# (3) Final Selected Threshold (ë³´ë¼ìƒ‰ êµµì€ ì„ )
fig.add_vline(x=selected_threshold, line_width=3, line_color="#AB63FA")
fig.add_annotation(
    x=selected_threshold, y=0.5,
    text=f"<b>Final Selection</b><br>(Avg: {selected_threshold:.4f})",
    showarrow=True, arrowhead=2,
    ax=-70, ay=0,
    bgcolor="rgba(255, 255, 255, 0.9)", bordercolor="#AB63FA", borderwidth=2
)

# ë ˆì´ì•„ì›ƒ ì„¤ì •
fig.update_layout(
    title='<b>Cross-Validation Threshold Check</b><br><span style="font-size:12px;color:gray">CV Precision, Recall, F2 & Final Decision</span>',
    xaxis_title='Threshold',
    yaxis_title='Score',
    yaxis=dict(range=[0, 1.1]),
    template='plotly_white',
    height=550,
    legend=dict(x=1.02, y=1, xanchor='left', yanchor='top', bgcolor='rgba(255, 255, 255, 0.5)'),
    margin=dict(r=150)
)

fig.show()
# -------------------------------------------------------
#  CV ê¸°ë°˜ ìµœì¢… ì„ê³„ê°’ í™•ì • ë° í•´ì„
# -------------------------------------------------------

# 1. Validation ê°’ ì—­ì‚°
# ê³µì‹: Final = (Val + CV) / 2  =>  Val = 2 * Final - CV
val_r80_est = 2 * selected_threshold - r80_cv_thr

print('='*70)
print('ğŸ“¢ [Cross-Validation ê¸°ë°˜] ìµœì¢… ì„ê³„ê°’ í™•ì • ë¦¬í¬íŠ¸')
print('='*70)

# 1ï¸âƒ£ CV F2 ìµœì  ì§€ì  (ì´ì „ ë³€ìˆ˜ f2_opt_thr ëŒ€ì‹  -> f2_cv_thr ì‚¬ìš©)
print(f"1ï¸âƒ£ CV ìµœì  ì„±ëŠ¥ ì§€ì  (Max F2-Score ê¸°ì¤€)")
print(f"   - ğŸ¯ ì„ê³„ê°’ (Threshold) : {f2_cv_thr:.4f}") 
print(f"   - ğŸ“ˆ F2-Score          : {max_f2_cv_score:.4f}")
print(f"   - ğŸ‘‰ í•´ì„: êµì°¨ê²€ì¦(CV) ê²°ê³¼, ì´ ì§€ì ì—ì„œ ëª¨ë¸ì˜ ì¢…í•© ì„±ëŠ¥ì´ ê°€ì¥ ë†’ìŠµë‹ˆë‹¤.")

# 2ï¸âƒ£ CV Recall 80% ì§€ì  (ì´ì „ ë³€ìˆ˜ r80_thr ëŒ€ì‹  -> r80_cv_thr ì‚¬ìš©)
print(f"\n2ï¸âƒ£ ê³ ìœ„í—˜êµ° íƒì§€ ê°•í™” ì§€ì  (CV Recall 80% ê¸°ì¤€)")
print(f"   - ğŸ¯ ì„ê³„ê°’ (Threshold) : {r80_cv_thr:.4f}") 
print(f"   - ğŸ‘‰ í•´ì„: í›ˆë ¨ ë°ì´í„° ì „ë°˜ì—ì„œ ì‹¤ì œ ë¶€ë„ì˜ 80%ë¥¼ ì¡ì•„ë‚´ëŠ” ë§ˆì§€ë…¸ì„ ì…ë‹ˆë‹¤.")
print(f"             Max F2 ì§€ì ë³´ë‹¤ ì•½ {f2_cv_thr - r80_cv_thr:.4f}ë§Œí¼ ë‚®ì¶° ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì¡ì€ ê°’ì…ë‹ˆë‹¤.")

print('-'*70)

# 3ï¸âƒ£ ìµœì¢… ê²°ì • (Selected Threshold)
print(f"3ï¸âƒ£ ìµœì¢… í™•ì • ì„ê³„ê°’ (Final Selection)")
print(f"   ğŸ‘‰ ê°’: {selected_threshold:.4f} (Val+CV í‰ê· )")
print(f"   (ì‚°ì¶œ ê·¼ê±°: Val R80% ({val_r80_est:.4f}) + CV R80% ({r80_cv_thr:.4f})ì˜ í‰ê· )")

print(f"\nğŸ’¡ [ì „ëµì  ì˜ë¯¸]")
diff = abs(val_r80_est - r80_cv_thr)
if diff < 0.05:
    print(f"   â€¢ Validationê³¼ CV ê²°ê³¼ê°€ ë§¤ìš° ìœ ì‚¬(ì°¨ì´ {diff:.4f})í•˜ì—¬ ëª¨ë¸ì´ **ì•ˆì •ì **ì…ë‹ˆë‹¤.")
else:
    print(f"   â€¢ Validationê³¼ CV ê²°ê³¼ì— ì°¨ì´({diff:.4f})ê°€ ìˆì–´ **í‰ê· ê°’ì„ ì‚¬ìš©í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€**í–ˆìŠµë‹ˆë‹¤.")

print(f"   â€¢ ìµœì¢…ì ìœ¼ë¡œ **Recall 80% ìˆ˜ì¤€ì˜ íƒì§€ë ¥ì„ ìœ ì§€**í•˜ë©´ì„œ,")
print(f"     íŠ¹ì • ë°ì´í„°ì…‹ì— ì¹˜ìš°ì¹˜ì§€ ì•ŠëŠ” **ì¼ë°˜í™”ëœ ì„ê³„ê°’**ì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
print('='*70)
```

    CV F2 ìµœì  ì„ê³„ê°’: 0.1964
    CV Recall 80% ì„ê³„ê°’: 0.0772
    âœ… ìµœì¢… ì„ê³„ê°’ (Val+CV í‰ê· ): 0.0856




    ======================================================================
    ğŸ“¢ [Cross-Validation ê¸°ë°˜] ìµœì¢… ì„ê³„ê°’ í™•ì • ë¦¬í¬íŠ¸
    ======================================================================
    1ï¸âƒ£ CV ìµœì  ì„±ëŠ¥ ì§€ì  (Max F2-Score ê¸°ì¤€)
       - ğŸ¯ ì„ê³„ê°’ (Threshold) : 0.1964
       - ğŸ“ˆ F2-Score          : 0.3066
       - ğŸ‘‰ í•´ì„: êµì°¨ê²€ì¦(CV) ê²°ê³¼, ì´ ì§€ì ì—ì„œ ëª¨ë¸ì˜ ì¢…í•© ì„±ëŠ¥ì´ ê°€ì¥ ë†’ìŠµë‹ˆë‹¤.
    
    2ï¸âƒ£ ê³ ìœ„í—˜êµ° íƒì§€ ê°•í™” ì§€ì  (CV Recall 80% ê¸°ì¤€)
       - ğŸ¯ ì„ê³„ê°’ (Threshold) : 0.0772
       - ğŸ‘‰ í•´ì„: í›ˆë ¨ ë°ì´í„° ì „ë°˜ì—ì„œ ì‹¤ì œ ë¶€ë„ì˜ 80%ë¥¼ ì¡ì•„ë‚´ëŠ” ë§ˆì§€ë…¸ì„ ì…ë‹ˆë‹¤.
                 Max F2 ì§€ì ë³´ë‹¤ ì•½ 0.1191ë§Œí¼ ë‚®ì¶° ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì¡ì€ ê°’ì…ë‹ˆë‹¤.
    ----------------------------------------------------------------------
    3ï¸âƒ£ ìµœì¢… í™•ì • ì„ê³„ê°’ (Final Selection)
       ğŸ‘‰ ê°’: 0.0856 (Val+CV í‰ê· )
       (ì‚°ì¶œ ê·¼ê±°: Val R80% (0.0940) + CV R80% (0.0772)ì˜ í‰ê· )
    
    ğŸ’¡ [ì „ëµì  ì˜ë¯¸]
       â€¢ Validationê³¼ CV ê²°ê³¼ê°€ ë§¤ìš° ìœ ì‚¬(ì°¨ì´ 0.0168)í•˜ì—¬ ëª¨ë¸ì´ **ì•ˆì •ì **ì…ë‹ˆë‹¤.
       â€¢ ìµœì¢…ì ìœ¼ë¡œ **Recall 80% ìˆ˜ì¤€ì˜ íƒì§€ë ¥ì„ ìœ ì§€**í•˜ë©´ì„œ,
         íŠ¹ì • ë°ì´í„°ì…‹ì— ì¹˜ìš°ì¹˜ì§€ ì•ŠëŠ” **ì¼ë°˜í™”ëœ ì„ê³„ê°’**ì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
    ======================================================================


## 7. Traffic Light ì‹œìŠ¤í…œ

### ğŸš¦ Traffic Light ë¦¬ìŠ¤í¬ ë“±ê¸‰í™” (3ë‹¨ê³„ ê´€ë¦¬ ì²´ê³„)

ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì—…ì„ 3ê°€ì§€ ë¦¬ìŠ¤í¬ ë“±ê¸‰ìœ¼ë¡œ ë¶„ë¥˜í•˜ì—¬, **ì‹¤ë¬´ì  ëŒ€ì‘ ìš°ì„ ìˆœìœ„**ë¥¼ ìˆ˜ë¦½í–ˆìŠµë‹ˆë‹¤.

1.  **ë“±ê¸‰ ê¸°ì¤€ (Thresholding)**:
    - ğŸ”´ **Red (ê³ ìœ„í—˜êµ°)**: ë¶€ë„ ê¸°ì—…ì˜ **80%**ë¥¼ í¬ì°©í•˜ëŠ” êµ¬ê°„ (Threshold $\ge$ 0.0940). ì¦‰ì‹œ ì •ë°€ ì§„ë‹¨ í•„ìš”.
    - ğŸŸ¡ **Yellow (ì ì¬ìœ„í—˜êµ°)**: ë¶€ë„ ê¸°ì—…ì˜ **95%**ê¹Œì§€ ì»¤ë²„í•˜ëŠ” ì•ˆì „ì¥ì¹˜ êµ¬ê°„ (Threshold $\ge$ 0.331). ëª¨ë‹ˆí„°ë§ ê°•í™”.
    - ğŸŸ¢ **Green (ì •ìƒêµ°)**: ë‚˜ë¨¸ì§€ í•˜ìœ„ êµ¬ê°„. ë¦¬ìŠ¤í¬ê°€ í˜„ì €íˆ ë‚®ìŒ.

2.  **ê²€ì¦ ê²°ê³¼ (Validation Set)**:
    - **Red ë“±ê¸‰**: ì „ì²´ì˜ ì•½ 18.8%ì¸ 1877ê°œ ê¸°ì—…ì„ ì„ ë³„í•˜ì—¬, ì‹¤ì œ ë¶€ë„ ê¸°ì—… 121ê°œë¥¼ ì§‘ì¤‘ í¬ì°©í–ˆìŠµë‹ˆë‹¤.
    - **Green ë“±ê¸‰**: 4454ê°œ ê¸°ì—… ì¤‘ ì‹¤ì œ ë¶€ë„ëŠ” ë‹¨ 7ê±´ì— ë¶ˆê³¼í•˜ì—¬, ëª¨ë¸ì˜ **ì•ˆì „ì„±(Safety)**ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.


```python
# Red: Recall 80%
red_threshold = r80_thr

# Yellow: Recall 95%
idx_95 = np.where(rec_v[:-1] >= 0.95)[0]
if len(idx_95) > 0:
    yellow_threshold = thr_v[idx_95[np.argmax(prec_v[:-1][idx_95])]]
else:
    yellow_threshold = thr_v[-1] if len(thr_v) > 0 else 0.01

print(f'Red >= {red_threshold:.4f}')
print(f'Yellow >= {yellow_threshold:.4f}')

def assign_traffic(prob, red, yellow):
    if prob >= red: return 'Red'
    elif prob >= yellow: return 'Yellow'
    else: return 'Green'

# Val í‰ê°€
traffic_val = pd.Series(y_val_prob_final).apply(lambda x: assign_traffic(x, red_threshold, yellow_threshold))
for grade in ['Red', 'Yellow', 'Green']:
    mask = (traffic_val == grade)
    cnt = mask.sum()
    bk = y_val.values[mask].sum()
    print(f'{grade}: {cnt} ê¸°ì—…, {bk} ë¶€ë„')
```

    Red >= 0.0940
    Yellow >= 0.0331
    Red: 1877 ê¸°ì—…, 121 ë¶€ë„
    Yellow: 3669 ê¸°ì—…, 23 ë¶€ë„
    Green: 4454 ê¸°ì—…, 7 ë¶€ë„



```python
# 1. ë°ì´í„° ì§‘ê³„ (ê¸°ì¡´ ë¡œì§ í™œìš©)
# -------------------------------------------------------
# Traffic Light í• ë‹¹
traffic_val = pd.Series(y_val_prob_final).apply(lambda x: assign_traffic(x, red_threshold, yellow_threshold))

# ë“±ê¸‰ë³„ í†µê³„ ê³„ì‚°
grades = ['Red', 'Yellow', 'Green']
grade_counts = []
grade_bks = []
grade_rates = []

for grade in grades:
    mask = (traffic_val == grade)
    cnt = mask.sum()
    bk = y_val.values[mask].sum()
    rate = bk / cnt if cnt > 0 else 0
    
    grade_counts.append(cnt)
    grade_bks.append(bk)
    grade_rates.append(rate)

# 2. ì‹œê°í™” (Bar Chart with Annotations)
# -------------------------------------------------------
# ìƒ‰ìƒ ì •ì˜ (Red, Yellow, Green)
colors = ['#EF553B', '#FECB52', '#00CC96']

fig = go.Figure()

# ë§‰ëŒ€ ê·¸ë˜í”„ (ê¸°ì—… ìˆ˜)
fig.add_trace(go.Bar(
    x=grades,
    y=grade_counts,
    marker_color=colors,
    text=[f"<b>{bk}</b> Bankruptcies<br>({rate:.1%})" for bk, rate in zip(grade_bks, grade_rates)],
    textposition='auto',
    name='Total Companies'
))

# ë ˆì´ì•„ì›ƒ ì„¤ì •
fig.update_layout(
    title=f'<b>Traffic Light Risk Distribution</b><br><span style="font-size:12px;color:gray">Red(Recall 80%) >= {red_threshold:.4f}, Yellow(Recall 95%) >= {yellow_threshold:.4f}</span>',
    xaxis_title='Risk Grade',
    yaxis_title='Number of Companies',
    template='plotly_white',
    height=500,
    showlegend=False
)

# ë¶€ë„ìœ¨(Risk Rate)ì„ ë³„ë„ í…ìŠ¤íŠ¸ë¡œ ìƒë‹¨ì— í‘œì‹œ (ì˜µì…˜)
for i, (cnt, rate) in enumerate(zip(grade_counts, grade_rates)):
    fig.add_annotation(
        x=grades[i], y=cnt,
        text=f"Total: {cnt:,}",
        showarrow=False,
        yshift=10,
        font=dict(color="black")
    )

fig.show()
# ----------------------------------------------------
# ğŸ’¡ Traffic Light Risk Distribution ê²°ê³¼ í•´ì„ ì½”ë“œ
# ----------------------------------------------------
total_bankruptcies = sum(grade_bks)

print("\n\n" + "=" * 60)
print("             ğŸš¦ Traffic Light Risk Distribution ê²°ê³¼ í•´ì„")
print("=" * 60)

# 2. Risk Gradeë³„ ì§‘ì¤‘ë„ ë¶„ì„
# 2.1. Red Zone í•´ì„
idx_red = grades.index('Red')
red_count = grade_counts[idx_red]
red_bks = grade_bks[idx_red]
red_rate = grade_rates[idx_red]
red_recall = red_bks / total_bankruptcies * 100 if total_bankruptcies > 0 else 0

print(f"#### ğŸ”´ Red Zone (ì¦‰ê° ì¡°ì¹˜ ì˜ì—­)")
print(f"  - ê¸°ì—… ìˆ˜: {red_count:,}ê°œ")
print(f"  - ë¶€ë„ ê¸°ì—… ìˆ˜: {red_bks:,}ê°œ (ì „ì²´ ë¶€ë„ì˜ {red_recall:.1f}%)")
print(f"  - ë¶€ë„ìœ¨ (Precision): {red_rate:.2%} (ê°€ì¥ ë†’ì€ ë¶€ë„ ì§‘ì¤‘ë„)")
print(f"  - í•´ì„: ì „ì²´ ê¸°ì—… ì¤‘ {red_count:,}ê°œì˜ ê¸°ì—…ì— ì „ì²´ ë¶€ë„ì˜ {red_recall:.1f}%ê°€ ì§‘ì¤‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ê¸°ì—…ë“¤ì€ ì¦‰ê°ì ì¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì¡°ì¹˜(ì˜ˆ: ëŒ€ì¶œ íšŒìˆ˜, ì‹¬ì¸µ ì¡°ì‚¬)ê°€ í•„ìš”í•œ ìµœìš°ì„  ê´€ë¦¬ ëŒ€ìƒì…ë‹ˆë‹¤.")

# 2.2. Yellow Zone í•´ì„
idx_yellow = grades.index('Yellow')
yellow_count = grade_counts[idx_yellow]
yellow_bks = grade_bks[idx_yellow]
yellow_rate = grade_rates[idx_yellow]
yellow_recall_total = (red_bks + yellow_bks) / total_bankruptcies * 100 if total_bankruptcies > 0 else 0

print(f"\n#### ğŸŸ¡ Yellow Zone (ê´€ì°° ë° ëª¨ë‹ˆí„°ë§ ì˜ì—­)")
print(f"  - ê¸°ì—… ìˆ˜* {yellow_count:,}ê°œ")
print(f"  - ë¶€ë„ ê¸°ì—… ìˆ˜: {yellow_bks:,}ê°œ")
print(f"  - ë¶€ë„ìœ¨: {yellow_rate:.2%}")
print(f"  - í•´ì„: Red + Yellow í•©ì‚° ì‹œ, ì „ì²´ ë¶€ë„ì˜ {yellow_recall_total:.1f}%ë¥¼ ì¡ì•„ëƒ…ë‹ˆë‹¤. Yellow Zone ê¸°ì—…ë“¤ì€ ì£¼ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ ë° ê²½ê³  ì‹ í˜¸ ë°œë ¹ì´ í•„ìš”í•œ ì ì¬ì  ìœ„í—˜ ëŒ€ìƒì…ë‹ˆë‹¤. ")

# 2.3. Green Zone í•´ì„
idx_green = grades.index('Green')
green_count = grade_counts[idx_green]
green_bks = grade_bks[idx_green]
green_rate = grade_rates[idx_green]
green_missed = green_bks / total_bankruptcies * 100 if total_bankruptcies > 0 else 0


print(f"\n#### ğŸŸ¢ Green Zone (ì •ìƒ ì˜ì—­)")
print(f"  - ê¸°ì—… ìˆ˜: {green_count:,}ê°œ")
print(f"  - ë¶€ë„ ê¸°ì—… ìˆ˜ (ë¯¸íƒì§€): {green_bks:,}ê°œ (ì „ì²´ ë¶€ë„ì˜ {green_missed:.1f}%)")
print(f"  - ë¶€ë„ìœ¨: {green_rate:.2%} (ë§¤ìš° ë‚®ìŒ)")
print("  - í•´ì„: ì „ì²´ ê¸°ì—…ì˜ ëŒ€ë¶€ë¶„ì´ í¬í•¨ë˜ë©°, ì´ ì˜ì—­ì—ì„œ ë°œìƒí•˜ëŠ” ë¶€ë„(ë¯¸íƒì§€)ëŠ” ê·¹íˆ ì ìŠµë‹ˆë‹¤. ì´ ê¸°ì—…ë“¤ì€ ìµœì†Œí•œì˜ ì •ê¸° ë³´ê³  ì™¸ì— ì¶”ê°€ì ì¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë¹„ìš©ì´ ê±°ì˜ ë°œìƒí•˜ì§€ ì•Šì•„ ìš´ì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.")
print("-" * 60)

print("### 3. ìµœì¢… í‰ê°€: ë¦¬ìŠ¤í¬ ì§‘ì¤‘ë„")
print(f"**ê²°ë¡ :** Traffic Light ì‹œìŠ¤í…œì€ ìœ„í—˜ ê¸°ì—…({total_bankruptcies:,}ê°œ)ì˜ ëŒ€ë¶€ë¶„({yellow_recall_total:.1f}%)ì„ Red ë° Yellow Zoneì´ë¼ëŠ” ì†Œìˆ˜ì˜ ê·¸ë£¹ì— ì„±ê³µì ìœ¼ë¡œ ì§‘ì¤‘ì‹œì¼œ, \në¦¬ìŠ¤í¬ ê´€ë¦¬ ìì›ì„ ë§¤ìš° íš¨ìœ¨ì ìœ¼ë¡œ ë°°ë¶„í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ì„ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤.")
print("=" * 60)
```



    
    
    ============================================================
                 ğŸš¦ Traffic Light Risk Distribution ê²°ê³¼ í•´ì„
    ============================================================
    #### ğŸ”´ Red Zone (ì¦‰ê° ì¡°ì¹˜ ì˜ì—­)
      - ê¸°ì—… ìˆ˜: 1,877ê°œ
      - ë¶€ë„ ê¸°ì—… ìˆ˜: 121ê°œ (ì „ì²´ ë¶€ë„ì˜ 80.1%)
      - ë¶€ë„ìœ¨ (Precision): 6.45% (ê°€ì¥ ë†’ì€ ë¶€ë„ ì§‘ì¤‘ë„)
      - í•´ì„: ì „ì²´ ê¸°ì—… ì¤‘ 1,877ê°œì˜ ê¸°ì—…ì— ì „ì²´ ë¶€ë„ì˜ 80.1%ê°€ ì§‘ì¤‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ê¸°ì—…ë“¤ì€ ì¦‰ê°ì ì¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì¡°ì¹˜(ì˜ˆ: ëŒ€ì¶œ íšŒìˆ˜, ì‹¬ì¸µ ì¡°ì‚¬)ê°€ í•„ìš”í•œ ìµœìš°ì„  ê´€ë¦¬ ëŒ€ìƒì…ë‹ˆë‹¤.
    
    #### ğŸŸ¡ Yellow Zone (ê´€ì°° ë° ëª¨ë‹ˆí„°ë§ ì˜ì—­)
      - ê¸°ì—… ìˆ˜* 3,669ê°œ
      - ë¶€ë„ ê¸°ì—… ìˆ˜: 23ê°œ
      - ë¶€ë„ìœ¨: 0.63%
      - í•´ì„: Red + Yellow í•©ì‚° ì‹œ, ì „ì²´ ë¶€ë„ì˜ 95.4%ë¥¼ ì¡ì•„ëƒ…ë‹ˆë‹¤. Yellow Zone ê¸°ì—…ë“¤ì€ ì£¼ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ ë° ê²½ê³  ì‹ í˜¸ ë°œë ¹ì´ í•„ìš”í•œ ì ì¬ì  ìœ„í—˜ ëŒ€ìƒì…ë‹ˆë‹¤. 
    
    #### ğŸŸ¢ Green Zone (ì •ìƒ ì˜ì—­)
      - ê¸°ì—… ìˆ˜: 4,454ê°œ
      - ë¶€ë„ ê¸°ì—… ìˆ˜ (ë¯¸íƒì§€): 7ê°œ (ì „ì²´ ë¶€ë„ì˜ 4.6%)
      - ë¶€ë„ìœ¨: 0.16% (ë§¤ìš° ë‚®ìŒ)
      - í•´ì„: ì „ì²´ ê¸°ì—…ì˜ ëŒ€ë¶€ë¶„ì´ í¬í•¨ë˜ë©°, ì´ ì˜ì—­ì—ì„œ ë°œìƒí•˜ëŠ” ë¶€ë„(ë¯¸íƒì§€)ëŠ” ê·¹íˆ ì ìŠµë‹ˆë‹¤. ì´ ê¸°ì—…ë“¤ì€ ìµœì†Œí•œì˜ ì •ê¸° ë³´ê³  ì™¸ì— ì¶”ê°€ì ì¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë¹„ìš©ì´ ê±°ì˜ ë°œìƒí•˜ì§€ ì•Šì•„ ìš´ì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    ------------------------------------------------------------
    ### 3. ìµœì¢… í‰ê°€: ë¦¬ìŠ¤í¬ ì§‘ì¤‘ë„
    **ê²°ë¡ :** Traffic Light ì‹œìŠ¤í…œì€ ìœ„í—˜ ê¸°ì—…(151ê°œ)ì˜ ëŒ€ë¶€ë¶„(95.4%)ì„ Red ë° Yellow Zoneì´ë¼ëŠ” ì†Œìˆ˜ì˜ ê·¸ë£¹ì— ì„±ê³µì ìœ¼ë¡œ ì§‘ì¤‘ì‹œì¼œ, 
    ë¦¬ìŠ¤í¬ ê´€ë¦¬ ìì›ì„ ë§¤ìš° íš¨ìœ¨ì ìœ¼ë¡œ ë°°ë¶„í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ì„ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤.
    ============================================================


## 8. â­ Test Set ìµœì¢… í‰ê°€ (Priority 2 - ì‹œê°í™”)

### 8.1 Test Set ì˜ˆì¸¡

### ğŸ¯ ìµœì¢… ê´€ë¬¸: Test Set ì„±ëŠ¥ í‰ê°€ (Generalization Check)

í•™ìŠµ ë° ê²€ì¦ ê³¼ì •ì— ì „í˜€ ê´€ì—¬í•˜ì§€ ì•Šì€ **Test Set(ì™„ì „í•œ ë¯¸ì§€ ë°ì´í„°)**ì„ í†µí•´ ëª¨ë¸ì˜ **ì¼ë°˜í™” ì„±ëŠ¥**ì„ ìµœì¢… ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

1.  **ì¡°ê¸° ê²½ë³´ ëŠ¥ë ¥ (Recall) ì…ì¦**:
    - **Recall 71.71%**: ì‹¤ì œ ë¶€ë„ ê¸°ì—… 152ê°œ ì¤‘ **109ê°œ**ë¥¼ ì‚¬ì „ì— í¬ì°©í•˜ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆë‹¤.
    - ë†“ì¹œ ê¸°ì—…(FN)ì€ ë‹¨ **43ê°œ**ì— ë¶ˆê³¼í•˜ì—¬, ê¸ˆìœµ ë¦¬ìŠ¤í¬ ê´€ë¦¬ì˜ í•µì‹¬ì¸ 'ì•ˆì „ì¥ì¹˜' ì—­í• ì„ ì¶©ì‹¤íˆ ìˆ˜í–‰í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

2.  **ëª¨ë¸ íš¨ìœ¨ì„± (PR-AUC)**:
    - **PR-AUC 0.1033**: ë¬´ì‘ìœ„ ì˜ˆì¸¡(Baseline ì•½ 1.5%) ëŒ€ë¹„ **ë°° ì´ìƒì˜ ì˜ˆì¸¡ íš¨ìœ¨**ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
    - **Trade-off**: ë†’ì€ Recall(71%)ì„ ìœ„í•´ ë‚®ì€ Precision(5.27%)ì„ ê°ìˆ˜í•˜ëŠ” ì „ëµì€, ë¶€ë„ ë¯¸íƒì§€ ë¹„ìš©ì´ ì˜¤íƒì§€ ë¹„ìš©ë³´ë‹¤ í›¨ì”¬ í° ë„ë©”ì¸ íŠ¹ì„±ìƒ í•©ë¦¬ì ì¸ ê²°ê³¼ì…ë‹ˆë‹¤.


```python
y_test_prob = final_model.predict_proba(X_test)[:, 1]
y_test_pred = (y_test_prob >= selected_threshold).astype(int)

test_pr_auc = average_precision_score(y_test, y_test_prob)
test_roc_auc = roc_auc_score(y_test, y_test_prob)
test_f2 = fbeta_score(y_test, y_test_pred, beta=2)

tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()
test_recall = tp / (tp + fn)
test_prec = tp / (tp + fp)

print('='*70)
print('ğŸ¯ Test Set ìµœì¢… í‰ê°€')
print('='*70)
print(f'PR-AUC: {test_pr_auc:.4f}')
print(f'ROC-AUC: {test_roc_auc:.4f}')
print(f'F2-Score: {test_f2:.4f}')
print(f'Precision: {test_prec:.2%}')
print(f'Recall: {test_recall:.2%}')
print(f'\nConfusion Matrix:')
print(f'  TN: {tn:,}  |  FP: {fp:,}')
print(f'  FN: {fn:,}  |  TP: {tp:,}')
print('='*70)

```

    ======================================================================
    ğŸ¯ Test Set ìµœì¢… í‰ê°€
    ======================================================================
    PR-AUC: 0.1033
    ROC-AUC: 0.8347
    F2-Score: 0.2036
    Precision: 5.27%
    Recall: 71.71%
    
    Confusion Matrix:
      TN: 7,888  |  FP: 1,960
      FN: 43  |  TP: 109
    ======================================================================



```python
# 1. ë°ì´í„° ì¤€ë¹„
metrics = ['PR-AUC', 'ROC-AUC', 'F2-Score', 'Precision', 'Recall']
values = [test_pr_auc, test_roc_auc, test_f2, test_prec, test_recall]
colors = ['#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A']

# 2. ì‹œê°í™” (Horizontal Bar Chart)
fig = go.Figure(go.Bar(
    x=values,
    y=metrics,
    orientation='h',
    text=[f'{v:.4f}' for v in values],  # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
    textposition='auto',
    marker_color=colors
))

# 3. ë ˆì´ì•„ì›ƒ ì„¤ì •
fig.update_layout(
    title='<b>Test Set Final Performance</b>',
    xaxis_title='Score',
    xaxis=dict(range=[0, 1.05]), # 0~1 ë²”ìœ„ ê³ ì •
    yaxis=dict(autorange="reversed"), # ìœ„ì—ì„œë¶€í„° ìˆœì„œëŒ€ë¡œ í‘œì‹œ
    template='plotly_white',
    height=400,
    showlegend=False
)

fig.show()

# ----------------------------------------------------
# ğŸ’¡ Test Set ìµœì¢… ì„±ëŠ¥ í•´ì„ ì½”ë“œ
# ----------------------------------------------------
print("\n\n" + "=" * 60)
print("             ğŸš€ Test Set ìµœì¢… ì„±ëŠ¥ ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„")
print("=" * 60)

# 1. í•µì‹¬ ì§€í‘œ ë¶„ì„
print("### 1. í•µì‹¬ ì¡°ê¸° ê²½ë³´ ì§€í‘œ (Recall & PR-AUC)")
print(f"ğŸ¥‡ Recall (ì¬í˜„ìœ¨): {test_recall:.2%}")
print(f"  - ì˜ë¯¸: ì‹¤ì œ ë¶€ë„ ê¸°ì—… ì¤‘ {test_recall:.2%}ë¥¼ ì‚¬ì „ì— ì •í™•íˆ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.")
print("  - í‰ê°€: ì´ ëª¨ë¸ì€ ë¶€ë„ ì˜ˆì¸¡ì˜ ìµœìš°ì„  ëª©í‘œì¸ 'ë¯¸íƒì§€(FN) ìµœì†Œí™”'ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì‹¤ì œ ë¶€ë„ ì†ì‹¤ì„ ë§‰ëŠ” ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œìœ¼ë¡œì„œì˜ ê°€ì¹˜ë¥¼ ì…ì¦í•©ë‹ˆë‹¤.")

print(f"ğŸ¥ˆ PR-AUC: {test_pr_auc:.4f}")
print("  - ì˜ë¯¸: ë¶ˆê· í˜• ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” ì¢…í•© ì§€í‘œì…ë‹ˆë‹¤.")
print("  - í‰ê°€: 0.1 ì´ìƒì˜ PR-AUCëŠ” ë§¤ìš° ë‚®ì€ ë¶€ë„ìœ¨(Positive Rate)ì„ ê³ ë ¤í–ˆì„ ë•Œ, ëª¨ë¸ì´ ì˜ˆì¸¡ê°’ ë‚´ì— ë¶€ë„ ìœ„í—˜ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì§‘ì¤‘ì‹œí‚¤ê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.")
print("-" * 60)

# 2. Confusion Matrix (ë¹„ìš© ë¶„ì„)
print("### 2. Confusion Matrixë¥¼ í†µí•œ ë¦¬ìŠ¤í¬ ë¹„ìš© ë¶„ì„")

total_bankruptcies = fn + tp

print(f"#### A. ì¹˜ëª…ì ì¸ ì‹¤íŒ¨ (FN - ë¯¸íƒì§€)")
print(f"  - FN (ë¯¸íƒì§€): {fn:,}ê±´")
print(f"  - í•´ì„: ëª¨ë¸ì´ ë¶€ë„ ê¸°ì—…ì„ ì •ìƒìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜í•œ ê±´ìˆ˜ì…ë‹ˆë‹¤. ì´ {fn:,}ê±´ì˜ ê¸°ì—…ì— ëŒ€í•´ì„œëŠ” ì‹¤ì œ ë¶€ë„ ì†ì‹¤ì´ ë°œìƒí•  ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. ")

print(f"#### B. ì˜¤ê²½ë³´ (FP - ì˜¤íƒì§€)")
print(f"  - FP (ì˜¤ê²½ë³´): {fp:,}ê±´")
print(f"  - í•´ì„: ëª¨ë¸ì´ ì •ìƒ ê¸°ì—…ì— ë¶€ë„ ê²½ê³ ë¥¼ ë‚´ë¦° ê±´ìˆ˜ì…ë‹ˆë‹¤. ì´ëŠ” ë¶ˆí•„ìš”í•œ ë¦¬ìŠ¤í¬ ê²€í†  ë° ìš´ì˜ ë¹„ìš©ì„ ë°œìƒì‹œí‚¤ì§€ë§Œ, ë¯¸íƒì§€ ë¹„ìš©(FN) ëŒ€ë¹„ ì¤‘ìš”ë„ê°€ ë‚®ìœ¼ë¯€ë¡œ í—ˆìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")

print(f"\n#### C. ìµœì¢… ìš”ì•½")
print(f"  - ìµœì¢… ì„ê³„ê°’({selected_threshold:.4f})ì€ Recallì„ ê·¹ëŒ€í™”í•˜ì—¬ ì „ì²´ ë¶€ë„ ê¸°ì—…({total_bankruptcies:,}ê±´) ì¤‘ {tp:,}ê±´ì„ ì„±ê³µì ìœ¼ë¡œ ê²½ë³´í•˜ëŠ” 'ì¡°ê¸° ê²½ë³´(Early Warning)' ì „ëµì— ë§ê²Œ ì‘ë™í–ˆìŒì„ í™•ì¸í•©ë‹ˆë‹¤.")
print("=" * 60)
```



    
    
    ============================================================
                 ğŸš€ Test Set ìµœì¢… ì„±ëŠ¥ ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„
    ============================================================
    ### 1. í•µì‹¬ ì¡°ê¸° ê²½ë³´ ì§€í‘œ (Recall & PR-AUC)
    ğŸ¥‡ Recall (ì¬í˜„ìœ¨): 71.71%
      - ì˜ë¯¸: ì‹¤ì œ ë¶€ë„ ê¸°ì—… ì¤‘ 71.71%ë¥¼ ì‚¬ì „ì— ì •í™•íˆ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.
      - í‰ê°€: ì´ ëª¨ë¸ì€ ë¶€ë„ ì˜ˆì¸¡ì˜ ìµœìš°ì„  ëª©í‘œì¸ 'ë¯¸íƒì§€(FN) ìµœì†Œí™”'ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì‹¤ì œ ë¶€ë„ ì†ì‹¤ì„ ë§‰ëŠ” ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œìœ¼ë¡œì„œì˜ ê°€ì¹˜ë¥¼ ì…ì¦í•©ë‹ˆë‹¤.
    ğŸ¥ˆ PR-AUC: 0.1033
      - ì˜ë¯¸: ë¶ˆê· í˜• ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” ì¢…í•© ì§€í‘œì…ë‹ˆë‹¤.
      - í‰ê°€: 0.1 ì´ìƒì˜ PR-AUCëŠ” ë§¤ìš° ë‚®ì€ ë¶€ë„ìœ¨(Positive Rate)ì„ ê³ ë ¤í–ˆì„ ë•Œ, ëª¨ë¸ì´ ì˜ˆì¸¡ê°’ ë‚´ì— ë¶€ë„ ìœ„í—˜ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì§‘ì¤‘ì‹œí‚¤ê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
    ------------------------------------------------------------
    ### 2. Confusion Matrixë¥¼ í†µí•œ ë¦¬ìŠ¤í¬ ë¹„ìš© ë¶„ì„
    #### A. ì¹˜ëª…ì ì¸ ì‹¤íŒ¨ (FN - ë¯¸íƒì§€)
      - FN (ë¯¸íƒì§€): 43ê±´
      - í•´ì„: ëª¨ë¸ì´ ë¶€ë„ ê¸°ì—…ì„ ì •ìƒìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜í•œ ê±´ìˆ˜ì…ë‹ˆë‹¤. ì´ 43ê±´ì˜ ê¸°ì—…ì— ëŒ€í•´ì„œëŠ” ì‹¤ì œ ë¶€ë„ ì†ì‹¤ì´ ë°œìƒí•  ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. 
    #### B. ì˜¤ê²½ë³´ (FP - ì˜¤íƒì§€)
      - FP (ì˜¤ê²½ë³´): 1,960ê±´
      - í•´ì„: ëª¨ë¸ì´ ì •ìƒ ê¸°ì—…ì— ë¶€ë„ ê²½ê³ ë¥¼ ë‚´ë¦° ê±´ìˆ˜ì…ë‹ˆë‹¤. ì´ëŠ” ë¶ˆí•„ìš”í•œ ë¦¬ìŠ¤í¬ ê²€í†  ë° ìš´ì˜ ë¹„ìš©ì„ ë°œìƒì‹œí‚¤ì§€ë§Œ, ë¯¸íƒì§€ ë¹„ìš©(FN) ëŒ€ë¹„ ì¤‘ìš”ë„ê°€ ë‚®ìœ¼ë¯€ë¡œ í—ˆìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.
    
    #### C. ìµœì¢… ìš”ì•½
      - ìµœì¢… ì„ê³„ê°’(0.0856)ì€ Recallì„ ê·¹ëŒ€í™”í•˜ì—¬ ì „ì²´ ë¶€ë„ ê¸°ì—…(152ê±´) ì¤‘ 109ê±´ì„ ì„±ê³µì ìœ¼ë¡œ ê²½ë³´í•˜ëŠ” 'ì¡°ê¸° ê²½ë³´(Early Warning)' ì „ëµì— ë§ê²Œ ì‘ë™í–ˆìŒì„ í™•ì¸í•©ë‹ˆë‹¤.
    ============================================================


### 8.2 â­ PR-AUC Curve ì‹œê°í™” (Priority 2)


```python
prec_test, rec_test, thr_test = precision_recall_curve(y_test, y_test_prob)

fig = go.Figure()
fig.add_trace(go.Scatter(x=rec_test, y=prec_test, mode='lines', name=f'PR-AUC={test_pr_auc:.4f}', line=dict(width=2)))
fig.add_trace(go.Scatter(x=[0, 1], y=[y_test.mean(), y_test.mean()], mode='lines', name='Baseline', line=dict(dash='dash')))
fig.update_layout(title='Precision-Recall Curve (Test Set)', xaxis_title='Recall', yaxis_title='Precision', height=500)
fig.show()

# ----------------------------------------------------
# ğŸ’¡ Test Set Precision-Recall Curve í•´ì„ ì½”ë“œ
# ----------------------------------------------------
print("\n\n" + "=" * 60)
print("              ğŸ“ˆ Test Set PR Curve ì‹œê°ì  í•´ì„")
print("=" * 60)

# 1. PR-AUC ê°’ í•´ì„
print("### 1. PR-AUC (Area Under the Curve) ë¶„ì„")
print(f"ğŸ¥‡ PR-AUC ê°’: {test_pr_auc:.4f}")
print(f"ğŸ¥ˆ Baseline (ë¬´ì‘ìœ„ ì˜ˆì¸¡) ê°’: {y_test.mean():.4f}")
print(f"- í‰ê°€: ëª¨ë¸ì˜ PR-AUC ({test_pr_auc:.4f})ê°€ ë¬´ì‘ìœ„ ì˜ˆì¸¡ ê¸°ì¤€ì„ ({y_test.mean():.4f})ë³´ë‹¤ í˜„ì €í•˜ê²Œ ë†’ìŠµë‹ˆë‹¤.")
print("  - ì´ëŠ” ëª¨ë¸ì´ ê·¹ì‹¬í•˜ê²Œ ë¶ˆê· í˜•í•œ Test Setì—ì„œë„ ë¶€ë„ ìœ„í—˜ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ìˆìŒì„ ê°•ë ¥í•˜ê²Œ ì…ì¦í•©ë‹ˆë‹¤.")
print("-" * 60)

# 2. Curve ëª¨ì–‘ í•´ì„ (ì‹œê°ì )
print("### 2. Curve ëª¨ì–‘ ë° Trade-off ë¶„ì„")
print("ğŸ‘‰ Curve ëª¨ì–‘: PR Curve(ì‹¤ì„ )ê°€ Baseline(ì ì„ )ìœ¼ë¡œë¶€í„° ë©€ë¦¬ ë–¨ì–´ì ¸ ìƒë‹¨ê³¼ ìš°ì¸¡ì— ìœ„ì¹˜í•©ë‹ˆë‹¤.")
print("  - Recallì´ ë‚®ì„ ë•Œ (ì¢Œì¸¡): Precision(ì •ë°€ë„) ê°’ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ì¦‰, ëª¨ë¸ì´ ë§¤ìš° í™•ì‹ í•˜ëŠ” ì˜ˆì¸¡ì€ ì‹¤ì œë¡œ ë¶€ë„ì¼ í™•ë¥ ì´ ë†’ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.")
print("  - Recallì´ ë†’ì•„ì§ˆ ë•Œ (ìš°ì¸¡): Recallì„ 80% ì´ìƒìœ¼ë¡œ ë†’ì´ëŠ” ê³¼ì •ì—ì„œ Precisionì€ í•˜ë½í•˜ëŠ” Trade-offê°€ ê´€ì°°ë©ë‹ˆë‹¤.")
print("  - ë¹„ì¦ˆë‹ˆìŠ¤ì  í•´ì„: ëª¨ë¸ì´ ë¯¸íƒì§€(FN)ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ Recallì„ ë†’ì´ëŠ” ì „ëµì„ ì„ íƒí•  ê²½ìš°, ì˜¤ê²½ë³´(FP) ì¦ê°€ë¼ëŠ” ë¹„ìš©ì„ ê°ìˆ˜í•´ì•¼ í•¨ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.")
print("=" * 60)
```



    
    
    ============================================================
                  ğŸ“ˆ Test Set PR Curve ì‹œê°ì  í•´ì„
    ============================================================
    ### 1. PR-AUC (Area Under the Curve) ë¶„ì„
    ğŸ¥‡ PR-AUC ê°’: 0.1033
    ğŸ¥ˆ Baseline (ë¬´ì‘ìœ„ ì˜ˆì¸¡) ê°’: 0.0152
    - í‰ê°€: ëª¨ë¸ì˜ PR-AUC (0.1033)ê°€ ë¬´ì‘ìœ„ ì˜ˆì¸¡ ê¸°ì¤€ì„ (0.0152)ë³´ë‹¤ í˜„ì €í•˜ê²Œ ë†’ìŠµë‹ˆë‹¤.
      - ì´ëŠ” ëª¨ë¸ì´ ê·¹ì‹¬í•˜ê²Œ ë¶ˆê· í˜•í•œ Test Setì—ì„œë„ ë¶€ë„ ìœ„í—˜ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ìˆìŒì„ ê°•ë ¥í•˜ê²Œ ì…ì¦í•©ë‹ˆë‹¤.
    ------------------------------------------------------------
    ### 2. Curve ëª¨ì–‘ ë° Trade-off ë¶„ì„
    ğŸ‘‰ Curve ëª¨ì–‘: PR Curve(ì‹¤ì„ )ê°€ Baseline(ì ì„ )ìœ¼ë¡œë¶€í„° ë©€ë¦¬ ë–¨ì–´ì ¸ ìƒë‹¨ê³¼ ìš°ì¸¡ì— ìœ„ì¹˜í•©ë‹ˆë‹¤.
      - Recallì´ ë‚®ì„ ë•Œ (ì¢Œì¸¡): Precision(ì •ë°€ë„) ê°’ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ì¦‰, ëª¨ë¸ì´ ë§¤ìš° í™•ì‹ í•˜ëŠ” ì˜ˆì¸¡ì€ ì‹¤ì œë¡œ ë¶€ë„ì¼ í™•ë¥ ì´ ë†’ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
      - Recallì´ ë†’ì•„ì§ˆ ë•Œ (ìš°ì¸¡): Recallì„ 80% ì´ìƒìœ¼ë¡œ ë†’ì´ëŠ” ê³¼ì •ì—ì„œ Precisionì€ í•˜ë½í•˜ëŠ” Trade-offê°€ ê´€ì°°ë©ë‹ˆë‹¤.
      - ë¹„ì¦ˆë‹ˆìŠ¤ì  í•´ì„: ëª¨ë¸ì´ ë¯¸íƒì§€(FN)ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ Recallì„ ë†’ì´ëŠ” ì „ëµì„ ì„ íƒí•  ê²½ìš°, ì˜¤ê²½ë³´(FP) ì¦ê°€ë¼ëŠ” ë¹„ìš©ì„ ê°ìˆ˜í•´ì•¼ í•¨ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
    ============================================================


### 8.3 â­ Confusion Matrix ì‹œê°í™” (Priority 2)


```python
cm = confusion_matrix(y_test, y_test_pred)
fig = go.Figure(data=go.Heatmap(z=cm, x=['Pred 0', 'Pred 1'], y=['True 0', 'True 1'], text=cm, texttemplate='%{text}', colorscale='Blues'))
fig.update_layout(title=f'Confusion Matrix (Threshold={selected_threshold:.4f})', height=400)
fig.show()
# ======================================================================
# ğŸ’¡ í•´ì„ ì¶œë ¥ ì½”ë“œ ì¶”ê°€
# ======================================================================
print("\n" + "=" * 70)
print(f"ğŸ“Š Confusion Matrix ìƒì„¸ í•´ì„ (Threshold: {selected_threshold:.4f})")
print("=" * 70)

# 1. ì„±ê³µì ì¸ ì˜ˆì¸¡ (True)
print("1ï¸âƒ£ ì„±ê³µì ì¸ ì˜ˆì¸¡:")
print(f" Â  - True Negative (TN): {tn:,}ê°œ  (ì •ìƒ ê¸°ì—…ì„ ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡) âœ…")
print(f" Â  - True Positive (TP): {tp:,}ê°œ  (ë¶€ë„ ê¸°ì—…ì„ ë¶€ë„ë¡œ ì˜ˆì¸¡) âœ… (ì†ì‹¤ íšŒí”¼ ì„±ê³µ)")

# 2. ì˜¤ë¶„ë¥˜ (False)
print("\n2ï¸âƒ£ ì˜¤ë¶„ë¥˜ (ëª¨ë¸ì˜ ì‹¤ìˆ˜):")
print(f" Â  - False Negative (FN): {fn:,}ê°œ  (ë¶€ë„ë¥¼ ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡) ğŸ”´ ê°€ì¥ ìœ„í—˜!")
print(f" Â  Â  â†’ ë†“ì¹œ ë¶€ë„: {fn:,}ê°œ. ì´ ê¸°ì—…ë“¤ì— ëŒ€í•œ ëŒ€ì¶œì€ ì‹¤ì œ ì†ì‹¤ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.")
print(f" Â  - False Positive (FP): {fp:,}ê°œ  (ì •ìƒì„ ë¶€ë„ë¡œ ì˜ˆì¸¡) âš ï¸ ê¸°íšŒ ì†ì‹¤ ë°œìƒ!")
print(f" Â  Â  â†’ ì˜¤ê²½ë³´: {fp:,}ê°œ. ì´ ê¸°ì—…ë“¤ì— ëŒ€í•œ ëŒ€ì¶œ ê±°ì ˆì€ ì´ì ìˆ˜ìµ í¬ê¸° (ê¸°íšŒ ì†ì‹¤)ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.")

# 3. ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬ìŠ¤í¬ ìš”ì•½
print("\n3ï¸âƒ£ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬ìŠ¤í¬ ìš”ì•½:")
print(f" Â  - Recall (ë¶€ë„ íƒì§€ìœ¨): {tp / (tp + fn) * 100:.1f}%")
print(f" Â  - Precision (ì •ë°€ë„): {tp / (tp + fp) * 100:.1f}%")
print(" Â  - í˜„ì¬ ì„ê³„ê°’ì€ ë†’ì€ Recallì„ ëª©í‘œë¡œ í–ˆìœ¼ë‚˜, Precision(ì˜¤ê²½ë³´)ì´ ë§¤ìš° ë‚®ì•„ ë¹„ì¦ˆë‹ˆìŠ¤ ìˆœ ìˆ˜ìµì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ì£¼ì—ˆìŠµë‹ˆë‹¤.")
print("=" * 70)
# ======================================================================
# ğŸ’¡ í•´ì„ ì¶œë ¥ ì½”ë“œ ì¶”ê°€
# ======================================================================
print("\n" + "=" * 70)
print(f"ğŸ“Š Confusion Matrix ìƒì„¸ í•´ì„ (Threshold: {selected_threshold:.4f})")
print("=" * 70)

# 1. ì„±ê³µì ì¸ ì˜ˆì¸¡ (True)
print("1ï¸âƒ£ ì„±ê³µì ì¸ ì˜ˆì¸¡:")
print(f" Â  - True Negative (TN): {tn:,}ê°œ  (ì •ìƒ ê¸°ì—…ì„ ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡) âœ…")
print(f" Â  - True Positive (TP): {tp:,}ê°œ  (ë¶€ë„ ê¸°ì—…ì„ ë¶€ë„ë¡œ ì˜ˆì¸¡) âœ… (ì†ì‹¤ íšŒí”¼ ì„±ê³µ)")

# 2. ì˜¤ë¶„ë¥˜ (False)
print("\n2ï¸âƒ£ ì˜¤ë¶„ë¥˜ (ëª¨ë¸ì˜ ì‹¤ìˆ˜):")
print(f" Â  - False Negative (FN): {fn:,}ê°œ  (ë¶€ë„ë¥¼ ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡) ğŸ”´ ê°€ì¥ ìœ„í—˜!")
print(f" Â  Â  â†’ ë†“ì¹œ ë¶€ë„: {fn:,}ê°œ. ì´ ê¸°ì—…ë“¤ì— ëŒ€í•œ ëŒ€ì¶œì€ ì‹¤ì œ ì†ì‹¤ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.")
print(f" Â  - False Positive (FP): {fp:,}ê°œ  (ì •ìƒì„ ë¶€ë„ë¡œ ì˜ˆì¸¡) âš ï¸ ê¸°íšŒ ì†ì‹¤ ë°œìƒ!")
print(f" Â  Â  â†’ ì˜¤ê²½ë³´: {fp:,}ê°œ. ì´ ê¸°ì—…ë“¤ì— ëŒ€í•œ ëŒ€ì¶œ ê±°ì ˆì€ ì´ì ìˆ˜ìµ í¬ê¸° (ê¸°íšŒ ì†ì‹¤)ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.")

# 3. ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬ìŠ¤í¬ ìš”ì•½
print("\n3ï¸âƒ£ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬ìŠ¤í¬ ìš”ì•½:")
print(f" Â  - Recall (ë¶€ë„ íƒì§€ìœ¨): {tp / (tp + fn) * 100:.1f}%")
print(f" Â  - Precision (ì •ë°€ë„): {tp / (tp + fp) * 100:.1f}%")
print(" Â  - í˜„ì¬ ì„ê³„ê°’ì€ ë†’ì€ Recallì„ ëª©í‘œë¡œ í–ˆìœ¼ë‚˜, Precision(ì˜¤ê²½ë³´)ì´ ë§¤ìš° ë‚®ì•„ ë¹„ì¦ˆë‹ˆìŠ¤ ìˆœ ìˆ˜ìµì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ì£¼ì—ˆìŠµë‹ˆë‹¤.")
print("=" * 70)
```



    
    ======================================================================
    ğŸ“Š Confusion Matrix ìƒì„¸ í•´ì„ (Threshold: 0.0856)
    ======================================================================
    1ï¸âƒ£ ì„±ê³µì ì¸ ì˜ˆì¸¡:
     Â  - True Negative (TN): 7,888ê°œ  (ì •ìƒ ê¸°ì—…ì„ ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡) âœ…
     Â  - True Positive (TP): 109ê°œ  (ë¶€ë„ ê¸°ì—…ì„ ë¶€ë„ë¡œ ì˜ˆì¸¡) âœ… (ì†ì‹¤ íšŒí”¼ ì„±ê³µ)
    
    2ï¸âƒ£ ì˜¤ë¶„ë¥˜ (ëª¨ë¸ì˜ ì‹¤ìˆ˜):
     Â  - False Negative (FN): 43ê°œ  (ë¶€ë„ë¥¼ ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡) ğŸ”´ **ê°€ì¥ ìœ„í—˜!**
     Â  Â  â†’ ë†“ì¹œ ë¶€ë„: 43ê°œ. ì´ ê¸°ì—…ë“¤ì— ëŒ€í•œ ëŒ€ì¶œì€ **ì‹¤ì œ ì†ì‹¤**ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.
     Â  - False Positive (FP): 1,960ê°œ  (ì •ìƒì„ ë¶€ë„ë¡œ ì˜ˆì¸¡) âš ï¸ **ê¸°íšŒ ì†ì‹¤ ë°œìƒ!**
     Â  Â  â†’ ì˜¤ê²½ë³´: 1,960ê°œ. ì´ ê¸°ì—…ë“¤ì— ëŒ€í•œ ëŒ€ì¶œ ê±°ì ˆì€ **ì´ì ìˆ˜ìµ í¬ê¸°** (ê¸°íšŒ ì†ì‹¤)ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.
    
    3ï¸âƒ£ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬ìŠ¤í¬ ìš”ì•½:
     Â  - **Recall (ë¶€ë„ íƒì§€ìœ¨):** 71.7%
     Â  - **Precision (ì •ë°€ë„):** 5.3%
     Â  - í˜„ì¬ ì„ê³„ê°’ì€ **ë†’ì€ Recall**ì„ ëª©í‘œë¡œ í–ˆìœ¼ë‚˜, **Precision(ì˜¤ê²½ë³´)ì´ ë§¤ìš° ë‚®ì•„** ë¹„ì¦ˆë‹ˆìŠ¤ ìˆœ ìˆ˜ìµì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ì£¼ì—ˆìŠµë‹ˆë‹¤.
    ======================================================================
    
    ======================================================================
    ğŸ“Š Confusion Matrix ìƒì„¸ í•´ì„ (Threshold: 0.0856)
    ======================================================================
    1ï¸âƒ£ ì„±ê³µì ì¸ ì˜ˆì¸¡:
     Â  - True Negative (TN): 7,888ê°œ  (ì •ìƒ ê¸°ì—…ì„ ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡) âœ…
     Â  - True Positive (TP): 109ê°œ  (ë¶€ë„ ê¸°ì—…ì„ ë¶€ë„ë¡œ ì˜ˆì¸¡) âœ… (ì†ì‹¤ íšŒí”¼ ì„±ê³µ)
    
    2ï¸âƒ£ ì˜¤ë¶„ë¥˜ (ëª¨ë¸ì˜ ì‹¤ìˆ˜):
     Â  - False Negative (FN): 43ê°œ  (ë¶€ë„ë¥¼ ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡) ğŸ”´ **ê°€ì¥ ìœ„í—˜!**
     Â  Â  â†’ ë†“ì¹œ ë¶€ë„: 43ê°œ. ì´ ê¸°ì—…ë“¤ì— ëŒ€í•œ ëŒ€ì¶œì€ **ì‹¤ì œ ì†ì‹¤**ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.
     Â  - False Positive (FP): 1,960ê°œ  (ì •ìƒì„ ë¶€ë„ë¡œ ì˜ˆì¸¡) âš ï¸ **ê¸°íšŒ ì†ì‹¤ ë°œìƒ!**
     Â  Â  â†’ ì˜¤ê²½ë³´: 1,960ê°œ. ì´ ê¸°ì—…ë“¤ì— ëŒ€í•œ ëŒ€ì¶œ ê±°ì ˆì€ **ì´ì ìˆ˜ìµ í¬ê¸°** (ê¸°íšŒ ì†ì‹¤)ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.
    
    3ï¸âƒ£ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬ìŠ¤í¬ ìš”ì•½:
     Â  - **Recall (ë¶€ë„ íƒì§€ìœ¨):** 71.7%
     Â  - **Precision (ì •ë°€ë„):** 5.3%
     Â  - í˜„ì¬ ì„ê³„ê°’ì€ **ë†’ì€ Recall**ì„ ëª©í‘œë¡œ í–ˆìœ¼ë‚˜, **Precision(ì˜¤ê²½ë³´)ì´ ë§¤ìš° ë‚®ì•„** ë¹„ì¦ˆë‹ˆìŠ¤ ìˆœ ìˆ˜ìµì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ì£¼ì—ˆìŠµë‹ˆë‹¤.
    ======================================================================


## 9. â­ ëª¨ë¸ ì§„ë‹¨ (Priority 3)

### 9.1 â­ Calibration Check (Priority 3)


```python
prob_true, prob_pred = calibration_curve(y_val, y_val_prob_final, n_bins=10, strategy='quantile')

fig = go.Figure()
fig.add_trace(go.Scatter(x=prob_pred, y=prob_true, mode='markers+lines', name='Model', marker=dict(size=10)))
fig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', name='Perfect', line=dict(dash='dash')))
fig.update_layout(title='Calibration Curve (Validation Set)', xaxis_title='Predicted Probability', yaxis_title='True Probability', height=500)
fig.show()
print('\nâœ… Calibration: ëŒ€ê°ì„ ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í™•ë¥  ì˜ˆì¸¡ ì •í™•')

# Calibration Curve í•´ì„ ì½”ë“œ
print("\n\n" + "=" * 60)
print("             ğŸ¯ Calibration Curve (í™•ë¥  ì •í•©ì„±) í•´ì„")
print("=" * 60)

print("### 1. Calibration Curveì˜ ëª©í‘œ")
print("ğŸ‘‰ ëª©í‘œ: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ ê°’(Xì¶•)ì´ ì‹¤ì œë¡œ í•´ë‹¹ í´ë˜ìŠ¤ì¼ í™•ë¥ (Yì¶•)ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ë©´ 'Perfect Line'(ëŒ€ê°ì„ ) ìœ„ì— ì ì´ ìœ„ì¹˜í•©ë‹ˆë‹¤.")
print("-" * 60)

print("### 2. ì‹œê°ì  ê²°ê³¼ ë¶„ì„")
print("#### A. Curveì˜ ìœ„ì¹˜ ë° ëª¨ì–‘")
print(" - ì´ìƒì ì¸ ìƒíƒœ: Curveê°€ Perfect Line(ì ì„ )ì— ë§¤ìš° ê°€ê¹ê²Œ ë¶™ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
print(" - ê´€ì°° ê²°ê³¼: Curveê°€ ëŒ€ê°ì„ ì— ê·¼ì ‘í•˜ê²Œ ìœ„ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.")

print("#### B. ì¼ë°˜ì ì¸ ê²½í–¥ì„± (ë¶€ë„ ì˜ˆì¸¡ ëª¨ë¸)")
print(" - Under-confidence (ë³´í†µì˜ í¬ë§ì ì¸ ê²°ê³¼): Curveê°€ Perfect Line ì•„ë˜ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš° (ì˜ˆì¸¡ í™•ë¥  < ì‹¤ì œ í™•ë¥ ) -> ëª¨ë¸ì´ ìœ„í—˜ í™•ë¥ ì„ ê³¼ì†Œí‰ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.")
print(" - Over-confidence (ìœ„í—˜í•œ ê²°ê³¼): Curveê°€ Perfect Line ìœ„ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš° (ì˜ˆì¸¡ í™•ë¥  > ì‹¤ì œ í™•ë¥ ) -> ëª¨ë¸ì´ ìœ„í—˜ í™•ë¥ ì„ ê³¼ëŒ€í‰ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.")
print(" - í‰ê°€: ì‹œê°ì  ê²°ê³¼ê°€ ëŒ€ê°ì„ ì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ëª¨ë¸ì˜ í™•ë¥  ì˜ˆì¸¡ì€ ìƒë‹¹íˆ ì •í•©ì (Well-calibrated)ì´ë©°, ì´í›„ ë‹¨ê³„ì˜ ë¦¬ìŠ¤í¬ ê¸ˆì•¡ ì‚°ì •ê³¼ Threshold ì„¤ì •ì˜ ê¸°ë°˜ìœ¼ë¡œì„œ ì‹ ë¢°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
print("=" * 60)
```



    
    âœ… Calibration: ëŒ€ê°ì„ ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í™•ë¥  ì˜ˆì¸¡ ì •í™•
    
    
    ============================================================
                 ğŸ¯ Calibration Curve (í™•ë¥  ì •í•©ì„±) í•´ì„
    ============================================================
    ### 1. Calibration Curveì˜ ëª©í‘œ
    ğŸ‘‰ ëª©í‘œ: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ ê°’(Xì¶•)ì´ ì‹¤ì œë¡œ í•´ë‹¹ í´ë˜ìŠ¤ì¼ í™•ë¥ (Yì¶•)ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ë©´ 'Perfect Line'(ëŒ€ê°ì„ ) ìœ„ì— ì ì´ ìœ„ì¹˜í•©ë‹ˆë‹¤.
    ------------------------------------------------------------
    ### 2. ì‹œê°ì  ê²°ê³¼ ë¶„ì„
    #### A. Curveì˜ ìœ„ì¹˜ ë° ëª¨ì–‘
     - ì´ìƒì ì¸ ìƒíƒœ: Curveê°€ Perfect Line(ì ì„ )ì— ë§¤ìš° ê°€ê¹ê²Œ ë¶™ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
     - ê´€ì°° ê²°ê³¼: Curveê°€ ëŒ€ê°ì„ ì— ê·¼ì ‘í•˜ê²Œ ìœ„ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    #### B. ì¼ë°˜ì ì¸ ê²½í–¥ì„± (ë¶€ë„ ì˜ˆì¸¡ ëª¨ë¸)
     - Under-confidence (ë³´í†µì˜ í¬ë§ì ì¸ ê²°ê³¼): Curveê°€ Perfect Line ì•„ë˜ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš° (ì˜ˆì¸¡ í™•ë¥  < ì‹¤ì œ í™•ë¥ ) -> ëª¨ë¸ì´ ìœ„í—˜ í™•ë¥ ì„ ê³¼ì†Œí‰ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.
     - Over-confidence (ìœ„í—˜í•œ ê²°ê³¼): Curveê°€ Perfect Line ìœ„ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš° (ì˜ˆì¸¡ í™•ë¥  > ì‹¤ì œ í™•ë¥ ) -> ëª¨ë¸ì´ ìœ„í—˜ í™•ë¥ ì„ ê³¼ëŒ€í‰ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.
     - í‰ê°€: ì‹œê°ì  ê²°ê³¼ê°€ ëŒ€ê°ì„ ì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ëª¨ë¸ì˜ í™•ë¥  ì˜ˆì¸¡ì€ ìƒë‹¹íˆ ì •í•©ì (Well-calibrated)ì´ë©°, ì´í›„ ë‹¨ê³„ì˜ ë¦¬ìŠ¤í¬ ê¸ˆì•¡ ì‚°ì •ê³¼ Threshold ì„¤ì •ì˜ ê¸°ë°˜ìœ¼ë¡œì„œ ì‹ ë¢°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ============================================================


### 9.2 â­ Learning Curve (Priority 3)


```python
train_sizes, train_scores, val_scores = learning_curve(
    final_model, X_train, y_train,
    cv=5, scoring=scorer,
    train_sizes=np.linspace(0.1, 1.0, 10),
    n_jobs=-1, random_state=RANDOM_STATE
)

fig = go.Figure()
fig.add_trace(go.Scatter(x=train_sizes, y=train_scores.mean(axis=1), mode='lines+markers', name='Train', error_y=dict(array=train_scores.std(axis=1))))
fig.add_trace(go.Scatter(x=train_sizes, y=val_scores.mean(axis=1), mode='lines+markers', name='CV', error_y=dict(array=val_scores.std(axis=1))))
fig.update_layout(title='Learning Curve', xaxis_title='Training Size', yaxis_title='PR-AUC', height=500)
fig.show()
print('\nâœ… Learning Curve: Trainê³¼ CV ê²©ì°¨ê°€ ì‘ìœ¼ë©´ ê³¼ì í•© ì—†ìŒ')
# Learning Curve í•´ì„ ì½”ë“œ
print("\n\n" + "=" * 60)
print("             ğŸ“ˆ Learning Curve (í•™ìŠµ ì•ˆì •ì„±) í•´ì„")
print("=" * 60)

print("### 1. í•™ìŠµ ê³¡ì„ ì˜ ëª©í‘œ ë° ì´ìƒì ì¸ í˜•íƒœ")
print("ğŸ‘‰ ëª©í‘œ: í›ˆë ¨ ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ëª¨ë¸ ì„±ëŠ¥ ë³€í™”ë¥¼ ê´€ì°°í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ê³¼ ì•ˆì •ì„±ì„ ì§„ë‹¨í•©ë‹ˆë‹¤.")
print("ğŸ‘‰ ì´ìƒì ì¸ í˜•íƒœ: í›ˆë ¨ ê³¡ì„ (Train)ê³¼ êµì°¨ ê²€ì¦ ê³¡ì„ (CV)ì´ ìˆ˜ë ´í•˜ê³ , ê·¸ ìˆ˜ë ´ëœ ê°’ì´ ë†’ì€ ì„±ëŠ¥(PR-AUC)ì„ ë³´ì´ëŠ” í˜•íƒœì…ë‹ˆë‹¤.")
print("-" * 60)

print("### 2. ì‹œê°ì  ê²°ê³¼ ë¶„ì„ ë° ì§„ë‹¨")

print("#### A. í›ˆë ¨ ê³¡ì„ (Train Score)")
print("  - í›ˆë ¨ ë°ì´í„° í¬ê¸°ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì ì  í•˜ë½í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.")
print("  - í•´ì„: ì´ëŠ” ëª¨ë¸ì´ ë” ë§ì€ ë°ì´í„°ì—ì„œ ì¼ë°˜ì ì¸ íŒ¨í„´ì„ í•™ìŠµí•˜ë©´ì„œ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ê³¼ì í•©ì´ ì ì°¨ í•´ì†Œë˜ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")

print("#### B. êµì°¨ ê²€ì¦ ê³¡ì„ (CV Score)")
print("  - CV ê³¡ì„ ì´ í›ˆë ¨ ë°ì´í„° í¬ê¸°ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì ì  ìƒìŠ¹í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.")
print("  - í•´ì„: ì´ëŠ” ëª¨ë¸ì´ ì¶”ê°€ì ì¸ í›ˆë ¨ ë°ì´í„°ë¥¼ í†µí•´ ì‹¤ì œ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")

print("#### C. ë‘ ê³¡ì„  ê°„ì˜ 'ê°„ê²©' (Gap)")
print("  - ê´€ì°°: í›ˆë ¨ ê³¡ì„ ê³¼ CV ê³¡ì„  ì‚¬ì´ì˜ ìµœì¢… ê°„ê²©ì„ í™•ì¸í•©ë‹ˆë‹¤.")
print("  - ì§„ë‹¨: ë§Œì•½ ë‘ ê³¡ì„ ì´ ì¢ì€ ê°„ê²©ì„ ìœ ì§€í•˜ë©° ìˆ˜ë ´í–ˆë‹¤ë©´, ëª¨ë¸ì€ ê³¼ì í•©(Overfitting) ì—†ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆë‹¤ê³  í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
print("-" * 60)

print("### 3. ì¶”ê°€ ë°ì´í„°ì˜ í•„ìš”ì„±")
print("ğŸ‘‰ íŒë‹¨: ë§Œì•½ ë‘ ê³¡ì„ ì´ ìˆ˜ë ´í•˜ì§€ ì•Šê³  CV ì ìˆ˜ê°€ ì—¬ì „íˆ ìƒìŠ¹ ì¶”ì„¸ë¼ë©´, ì¶”ê°€ì ì¸ ë°ì´í„°ë¥¼ í™•ë³´í•˜ì—¬ í•™ìŠµì— ì‚¬ìš©í–ˆì„ ë•Œ ëª¨ë¸ ì„±ëŠ¥ì„ ë” í–¥ìƒì‹œí‚¬ ì—¬ì§€ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")
print("=" * 60)
```



    
    âœ… Learning Curve: Trainê³¼ CV ê²©ì°¨ê°€ ì‘ìœ¼ë©´ ê³¼ì í•© ì—†ìŒ
    
    
    ============================================================
                 ğŸ“ˆ Learning Curve (í•™ìŠµ ì•ˆì •ì„±) í•´ì„
    ============================================================
    ### 1. í•™ìŠµ ê³¡ì„ ì˜ ëª©í‘œ ë° ì´ìƒì ì¸ í˜•íƒœ
    ğŸ‘‰ ëª©í‘œ: í›ˆë ¨ ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ëª¨ë¸ ì„±ëŠ¥ ë³€í™”ë¥¼ ê´€ì°°í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ê³¼ ì•ˆì •ì„±ì„ ì§„ë‹¨í•©ë‹ˆë‹¤.
    ğŸ‘‰ ì´ìƒì ì¸ í˜•íƒœ: í›ˆë ¨ ê³¡ì„ (Train)ê³¼ êµì°¨ ê²€ì¦ ê³¡ì„ (CV)ì´ ìˆ˜ë ´í•˜ê³ , ê·¸ ìˆ˜ë ´ëœ ê°’ì´ ë†’ì€ ì„±ëŠ¥(PR-AUC)ì„ ë³´ì´ëŠ” í˜•íƒœì…ë‹ˆë‹¤.
    ------------------------------------------------------------
    ### 2. ì‹œê°ì  ê²°ê³¼ ë¶„ì„ ë° ì§„ë‹¨
    #### A. í›ˆë ¨ ê³¡ì„ (Train Score)
      - í›ˆë ¨ ë°ì´í„° í¬ê¸°ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì ì  í•˜ë½í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
      - í•´ì„: ì´ëŠ” ëª¨ë¸ì´ ë” ë§ì€ ë°ì´í„°ì—ì„œ ì¼ë°˜ì ì¸ íŒ¨í„´ì„ í•™ìŠµí•˜ë©´ì„œ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ê³¼ì í•©ì´ ì ì°¨ í•´ì†Œë˜ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    #### B. êµì°¨ ê²€ì¦ ê³¡ì„ (CV Score)
      - CV ê³¡ì„ ì´ í›ˆë ¨ ë°ì´í„° í¬ê¸°ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì ì  ìƒìŠ¹í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
      - í•´ì„: ì´ëŠ” ëª¨ë¸ì´ ì¶”ê°€ì ì¸ í›ˆë ¨ ë°ì´í„°ë¥¼ í†µí•´ ì‹¤ì œ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    #### C. ë‘ ê³¡ì„  ê°„ì˜ 'ê°„ê²©' (Gap)
      - ê´€ì°°: í›ˆë ¨ ê³¡ì„ ê³¼ CV ê³¡ì„  ì‚¬ì´ì˜ ìµœì¢… ê°„ê²©ì„ í™•ì¸í•©ë‹ˆë‹¤.
      - ì§„ë‹¨: ë§Œì•½ ë‘ ê³¡ì„ ì´ ì¢ì€ ê°„ê²©ì„ ìœ ì§€í•˜ë©° ìˆ˜ë ´í–ˆë‹¤ë©´, ëª¨ë¸ì€ ê³¼ì í•©(Overfitting) ì—†ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆë‹¤ê³  í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ------------------------------------------------------------
    ### 3. ì¶”ê°€ ë°ì´í„°ì˜ í•„ìš”ì„±
    ğŸ‘‰ íŒë‹¨: ë§Œì•½ ë‘ ê³¡ì„ ì´ ìˆ˜ë ´í•˜ì§€ ì•Šê³  CV ì ìˆ˜ê°€ ì—¬ì „íˆ ìƒìŠ¹ ì¶”ì„¸ë¼ë©´, ì¶”ê°€ì ì¸ ë°ì´í„°ë¥¼ í™•ë³´í•˜ì—¬ í•™ìŠµì— ì‚¬ìš©í–ˆì„ ë•Œ ëª¨ë¸ ì„±ëŠ¥ì„ ë” í–¥ìƒì‹œí‚¬ ì—¬ì§€ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    ============================================================


## 10. â­ Cumulative Gains Curve (Priority 2)


```python
# Test Set Cumulative Gains
df_gains = pd.DataFrame({'prob': y_test_prob, 'y': y_test.values})
df_gains = df_gains.sort_values('prob', ascending=False).reset_index(drop=True)
df_gains['cum_y'] = df_gains['y'].cumsum()
df_gains['pct_captured'] = df_gains['cum_y'] / df_gains['y'].sum()
df_gains['pct_population'] = (df_gains.index + 1) / len(df_gains)

fig = go.Figure()
fig.add_trace(go.Scatter(x=df_gains['pct_population'], y=df_gains['pct_captured'], mode='lines', name='Model', line=dict(width=2)))
fig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', name='Random', line=dict(dash='dash')))
fig.update_layout(title='Cumulative Gains Curve (Test Set)', xaxis_title='% Population', yaxis_title='% Bankruptcies Captured', xaxis_tickformat='.0%', yaxis_tickformat='.0%', height=500)
fig.show()

# ìƒìœ„ 10% í¬ì°©ë¥ 
top10_captured = df_gains[df_gains['pct_population'] <= 0.1]['pct_captured'].iloc[-1]
print(f'\nâœ… ìƒìœ„ 10% ê¸°ì—…ì—ì„œ ë¶€ë„ì˜ {top10_captured:.1%} í¬ì°©')
print(f'   íš¨ìœ¨ì„±: Random ëŒ€ë¹„ {top10_captured/0.1:.1f}ë°°')
# ======================================================================
# ğŸ’¡ í•´ì„ ì¶œë ¥ ì½”ë“œ ì¶”ê°€
# ======================================================================
print("\n" + "=" * 70)
print("ğŸ¯ Cumulative Gains Curve í•´ì„: íƒ€ê²ŸíŒ… íš¨ìœ¨ì„±")
print("=" * 70)
print("1. ğŸ“ˆ ê³¡ì„ ì˜ ì˜ë¯¸:")
print("   - Xì¶• (% Population): ëª¨ë¸ì´ ê³ ìœ„í—˜ìœ¼ë¡œ íŒë‹¨í•œ ê¸°ì—…ì˜ ëˆ„ì  ë¹„ìœ¨ (ê²€í†  ìì›)")
print("   - Yì¶• (% Bankruptcies Captured): Xì¶• ê¸°ì—…ë“¤ ì†ì— ì‹¤ì œ ë¶€ë„ ê¸°ì—…ì´ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€ (í¬ì°©ë¥ )")
print("2. âœ… íƒ€ê²ŸíŒ… íš¨ìœ¨ì„± (í•µì‹¬ ì§€í‘œ):")
print(f"   - í•µì‹¬ ì§€ì  (ìƒìœ„ 10%): ì „ì²´ ê¸°ì—…ì˜ 10%ë§Œì„ ê²€í† í–ˆì„ ë•Œ, ì‹¤ì œ ë¶€ë„ ê¸°ì—…ì˜ {top10_captured:.1%}**ë¥¼ í¬ì°©í•©ë‹ˆë‹¤.")
print(f"   - ì„ ë³„ ëŠ¥ë ¥: ì´ëŠ” ë¬´ì‘ìœ„ ì„ íƒ({0.1:.1%}) ëŒ€ë¹„ {top10_captured/0.1:.1f}ë°° ë†’ì€ íš¨ìœ¨ì„±ì„ ì˜ë¯¸í•˜ë©°, ëª¨ë¸ì˜ ì„ ë³„ë ¥ì´ ë§¤ìš° ìš°ìˆ˜í•¨ì„ ì¦ëª…í•©ë‹ˆë‹¤.")
print("3. ğŸ’° ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œì‚¬ì  (ìì› ë°°ë¶„):")
print("   - ëª¨ë¸ì„ í†µí•´ ìœ„í—˜ë„ê°€ ë†’ì€ ìƒìœ„ 10%ì˜ ê¸°ì—…ì—ë§Œ ì œí•œëœ ì‹¬ì‚¬ ìì›ì„ ì§‘ì¤‘í•  ê²½ìš°, ì „ì²´ ë¶€ë„ ê¸°ì—…ì˜ ì ˆë°˜ ê°€ê¹Œì´ë¥¼ ì‚¬ì „ì— ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
print("   - **ìì› ë‚­ë¹„ë¥¼ ìµœì†Œí™”í•˜ê³ , íš¨ìœ¨ì ìœ¼ë¡œ ê³ ìœ„í—˜êµ°ì— ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” ëª…í™•í•œ ê·¼ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
print("=" * 70)

```



    
    âœ… ìƒìœ„ 10% ê¸°ì—…ì—ì„œ ë¶€ë„ì˜ 47.4% í¬ì°©
       íš¨ìœ¨ì„±: Random ëŒ€ë¹„ 4.7ë°°
    
    ======================================================================
    ğŸ¯ Cumulative Gains Curve í•´ì„: íƒ€ê²ŸíŒ… íš¨ìœ¨ì„±
    ======================================================================
    1. ğŸ“ˆ ê³¡ì„ ì˜ ì˜ë¯¸:
       - Xì¶• (% Population): ëª¨ë¸ì´ ê³ ìœ„í—˜ìœ¼ë¡œ íŒë‹¨í•œ ê¸°ì—…ì˜ ëˆ„ì  ë¹„ìœ¨ (ê²€í†  ìì›)
       - Yì¶• (% Bankruptcies Captured): Xì¶• ê¸°ì—…ë“¤ ì†ì— ì‹¤ì œ ë¶€ë„ ê¸°ì—…ì´ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€ (í¬ì°©ë¥ )
    2. âœ… íƒ€ê²ŸíŒ… íš¨ìœ¨ì„± (í•µì‹¬ ì§€í‘œ):
       - **í•µì‹¬ ì§€ì  (ìƒìœ„ 10%):** ì „ì²´ ê¸°ì—…ì˜ 10%ë§Œì„ ê²€í† í–ˆì„ ë•Œ, ì‹¤ì œ ë¶€ë„ ê¸°ì—…ì˜ **47.4%**ë¥¼ í¬ì°©í•©ë‹ˆë‹¤.
       - **ì„ ë³„ ëŠ¥ë ¥:** ì´ëŠ” ë¬´ì‘ìœ„ ì„ íƒ(10.0%) ëŒ€ë¹„ **4.7ë°°** ë†’ì€ íš¨ìœ¨ì„±ì„ ì˜ë¯¸í•˜ë©°, ëª¨ë¸ì˜ **ì„ ë³„ë ¥**ì´ ë§¤ìš° ìš°ìˆ˜í•¨ì„ ì¦ëª…í•©ë‹ˆë‹¤.
    3. ğŸ’° ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œì‚¬ì  (ìì› ë°°ë¶„):
       - ëª¨ë¸ì„ í†µí•´ ìœ„í—˜ë„ê°€ ë†’ì€ ìƒìœ„ 10%ì˜ ê¸°ì—…ì—ë§Œ **ì œí•œëœ ì‹¬ì‚¬ ìì›**ì„ ì§‘ì¤‘í•  ê²½ìš°, **ì „ì²´ ë¶€ë„ ê¸°ì—…ì˜ ì ˆë°˜ ê°€ê¹Œì´**ë¥¼ ì‚¬ì „ì— ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
       - **ìì› ë‚­ë¹„ë¥¼ ìµœì†Œí™”**í•˜ê³ , íš¨ìœ¨ì ìœ¼ë¡œ **ê³ ìœ„í—˜êµ°ì— ì§‘ì¤‘**í•  ìˆ˜ ìˆëŠ” ëª…í™•í•œ ê·¼ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    ======================================================================


## 11. Feature Importance

### ğŸ“Š ëª¨ë¸ í•´ì„: ì£¼ìš” ë³€ìˆ˜ ì¤‘ìš”ë„ (Feature Importance) ë¶„ì„

ìµœì¢… ì„ ì •ëœ ëª¨ë¸ì´ ì–´ë–¤ ë³€ìˆ˜ì— ê¸°ë°˜í•˜ì—¬ ë¶€ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ **ë³€ìˆ˜ ì¤‘ìš”ë„**ë¥¼ ì‹œê°í™”í–ˆìŠµë‹ˆë‹¤.

1.  **ë¶„ì„ ë°©ë²•**:
    - **Tree ê¸°ë°˜ ëª¨ë¸ (CatBoost ë“±)**: ë¶ˆìˆœë„(Gini Impurity) ê°ì†Œ ê¸°ì—¬ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 15ê°œ í•µì‹¬ ë³€ìˆ˜ë¥¼ ì¶”ì¶œí•˜ì—¬ `Viridis` ì»¬ëŸ¬ ìŠ¤ì¼€ì¼ë¡œ í‘œí˜„.
    - **ì„ í˜• ëª¨ë¸ (Logistic Regression)**: ê° ë³€ìˆ˜ì˜ íšŒê·€ ê³„ìˆ˜(Coefficient)ë¥¼ ì¶”ì¶œí•˜ì—¬, ë¶€ë„ ìœ„í—˜ì„ ë†’ì´ëŠ” ë³€ìˆ˜(**Red**)ì™€ ë‚®ì¶”ëŠ” ë³€ìˆ˜(**Blue**)ë¥¼ ëª…í™•íˆ ëŒ€ë¹„.

2.  **ë¶„ì„ ëª©ì **:
    - ëª¨ë¸ì˜ **ì„¤ëª… ê°€ëŠ¥ì„±(Explainability)** í™•ë³´.
    - ì¬ë¬´ì  ìƒì‹ê³¼ ëª¨ë¸ì˜ í•™ìŠµ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦ (ì˜ˆ: ìœ ë™ë¹„ìœ¨ì´ ë‚®ì„ìˆ˜ë¡ ìœ„í—˜í•œê°€?).


```python
if final_name == 'VotingEnsemble':
    # Ensembleì´ë©´ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©
    clf = final_model.estimators_[0].named_steps['clf']
else:
    clf = final_model.named_steps['clf']

# 1. Tree ê¸°ë°˜ ëª¨ë¸ Feature Importance (Viridis ìŠ¤ì¼€ì¼ ì ìš©)
if hasattr(clf, 'feature_importances_'):
    imp = clf.feature_importances_
    feat_imp = pd.DataFrame({'Feature': X_train.columns, 'Importance': imp}).sort_values('Importance', ascending=False).head(15)
    
    fig = go.Figure(go.Bar(
        x=feat_imp['Importance'], 
        y=feat_imp['Feature'], 
        orientation='h', 
        marker=dict(
            color=feat_imp['Importance'], 
            colorscale='Viridis' # [ìˆ˜ì •] Bluesë³´ë‹¤ ëª…í™•í•œ ëŒ€ë¹„ë¥¼ ì£¼ëŠ” ìŠ¤ì¼€ì¼
        )
    ))
    fig.update_layout(title=f'Top 15 Feature Importance ({final_name})', xaxis_title='Importance', yaxis_title='Feature', height=600, yaxis={'categoryorder':'total ascending'})
    fig.show()
else:
    print('Feature Importance ì§€ì› ì•ˆ í•¨')

# 2. LogisticRegression ê³„ìˆ˜ í™•ì¸ (ì–‘ìˆ˜/ìŒìˆ˜ ë‹¨ìƒ‰ ëŒ€ë¹„ ì ìš©)
if final_name == 'LogisticRegression':
    # íŒŒì´í”„ë¼ì¸ì—ì„œ ë¶„ë¥˜ê¸°(clf) ë‹¨ê³„ ê°€ì ¸ì˜¤ê¸°
    clf = final_model.named_steps['clf']
    
    # ê³„ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    coefficients = clf.coef_[0]
    features = X_train.columns
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    coef_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})
    
    # ì ˆëŒ€ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì˜í–¥ë ¥ì´ í° ìˆœì„œëŒ€ë¡œ)
    coef_df['Abs_Coef'] = coef_df['Coefficient'].abs()
    coef_df = coef_df.sort_values(by='Abs_Coef', ascending=False).head(20)
    
    # [ìˆ˜ì •] 0ì„ ê¸°ì¤€ìœ¼ë¡œ ì–‘ìˆ˜(Red), ìŒìˆ˜(Blue) ë‹¨ìƒ‰ ì§€ì • (ëª…í™•í•œ ëŒ€ë¹„)
    colors = ['#EF553B' if c > 0 else '#636EFA' for c in coef_df['Coefficient']]
    
    # ì‹œê°í™”
    fig = go.Figure()
    fig.add_trace(go.Bar(
        y=coef_df['Feature'],
        x=coef_df['Coefficient'],
        orientation='h',
        marker=dict(color=colors) # colorscale ëŒ€ì‹  ì§ì ‘ ì§€ì •í•œ ìƒ‰ìƒ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
    ))
    
    fig.update_layout(
        title=f'Top 20 Coefficients (Impact) - {final_name}<br><span style="font-size:12px;color:gray">Red: Increases Risk (+), Blue: Decreases Risk (-)</span>',
        xaxis_title='Coefficient Value (Log-Odds Impact)',
        yaxis=dict(categoryorder='total ascending'),
        height=600
    )
    fig.show()
    # ----------------------------------------------------
# ğŸ’¡ Feature Importance/Coefficient ê²°ê³¼ í•´ì„ ì½”ë“œ
# ----------------------------------------------------
import pandas as pd
import numpy as np

print("\n\n" + "=" * 60)
print(f"             ğŸ“Š {final_name} ëª¨ë¸ íŠ¹ì§• ì¤‘ìš”ë„ í•´ì„")
print("=" * 60)

# 1. Tree-based Model (Feature Importance) í•´ì„
if hasattr(clf, 'feature_importances_'):
    # Top 15 íŠ¹ì„± ì¤‘ìš”ë„ ë°ì´í„°í”„ë ˆì„ ì¬ì •ì˜ (í•´ì„ì„ ìœ„í•´)
    imp = clf.feature_importances_
    feat_imp = pd.DataFrame({'Feature': X_train.columns, 'Importance': imp}).sort_values('Importance', ascending=False)
    
    top3_imp = feat_imp.head(3)
    
    print("### 1. ğŸŒ² Tree ê¸°ë°˜ ëª¨ë¸ (Importance) í•´ì„")
    print(f"ğŸ‘‰ ìµœì¢… ëª¨ë¸ ({final_name})ì€ ë‹¤ìŒ Featureë“¤ì„ ë¦¬ìŠ¤í¬ ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”í•˜ê²Œ í™œìš©í–ˆìŠµë‹ˆë‹¤.")
    
    print("\n#### ğŸ¥‡ Top 3 ì£¼ìš” ë¦¬ìŠ¤í¬ ê²°ì • ìš”ì¸ (ì¤‘ìš”ë„ ìˆœ):")
    for i, row in top3_imp.iterrows():
        print(f"  - {i+1}. {row['Feature']} (ì¤‘ìš”ë„: {row['Importance']:.4f})")
    
    print("\n#### ë¹„ì¦ˆë‹ˆìŠ¤ì  í•´ì„:")
    print("- ì¤‘ìš”ë„ ì˜ë¯¸: ì´ Featureë“¤ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë° ê°€ì¥ í¬ê²Œ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤ (ë…¸ë“œ ë¶„í•  ì‹œ ë¶ˆìˆœë„ ê°ì†Œì— ê°€ì¥ íš¨ê³¼ì ).")
    print("- í™œìš©: ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹œ, ì´ Top Featureë“¤ì˜ ìˆ˜ì¹˜ ë³€í™”ë¥¼ ê°€ì¥ ë©´ë°€í•˜ê²Œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ê²ƒì´ ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì„ ë†’ì´ëŠ” í•µì‹¬ì…ë‹ˆë‹¤. ")

# 2. Logistic Regression (Coefficient) í•´ì„
elif final_name == 'LogisticRegression':
    # Top 20 ê³„ìˆ˜ ë°ì´í„°í”„ë ˆì„ ì¬ì •ì˜ (í•´ì„ì„ ìœ„í•´)
    coefficients = clf.coef_[0]
    features = X_train.columns
    coef_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})
    coef_df['Abs_Coef'] = coef_df['Coefficient'].abs()
    coef_df = coef_df.sort_values(by='Abs_Coef', ascending=False)
    
    top3_pos = coef_df[coef_df['Coefficient'] > 0].head(3)
    top3_neg = coef_df[coef_df['Coefficient'] < 0].head(3)
    
    
```



    
    
    ============================================================
                 ğŸ“Š CatBoost ëª¨ë¸ íŠ¹ì§• ì¤‘ìš”ë„ í•´ì„
    ============================================================
    ### 1. ğŸŒ² Tree ê¸°ë°˜ ëª¨ë¸ (Importance) í•´ì„
    ğŸ‘‰ ìµœì¢… ëª¨ë¸ (CatBoost)ì€ ë‹¤ìŒ Featureë“¤ì„ ë¦¬ìŠ¤í¬ ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”í•˜ê²Œ í™œìš©í–ˆìŠµë‹ˆë‹¤.
    
    #### ğŸ¥‡ Top 3 ì£¼ìš” ë¦¬ìŠ¤í¬ ê²°ì • ìš”ì¸ (ì¤‘ìš”ë„ ìˆœ):
      - 6. ì‹ ìš©ë“±ê¸‰ì ìˆ˜ (ì¤‘ìš”ë„: 64.4921)
      - 9. ê³µê³µì •ë³´ë¦¬ìŠ¤í¬ (ì¤‘ìš”ë„: 9.0211)
      - 23. ì¬ê³ ë³´ìœ ì¼ìˆ˜ (ì¤‘ìš”ë„: 6.9653)
    
    #### ë¹„ì¦ˆë‹ˆìŠ¤ì  í•´ì„:
    - ì¤‘ìš”ë„ ì˜ë¯¸: ì´ Featureë“¤ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë° ê°€ì¥ í¬ê²Œ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤ (ë…¸ë“œ ë¶„í•  ì‹œ ë¶ˆìˆœë„ ê°ì†Œì— ê°€ì¥ íš¨ê³¼ì ).
    - í™œìš©: ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹œ, ì´ Top Featureë“¤ì˜ ìˆ˜ì¹˜ ë³€í™”ë¥¼ ê°€ì¥ ë©´ë°€í•˜ê²Œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ê²ƒì´ ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì„ ë†’ì´ëŠ” í•µì‹¬ì…ë‹ˆë‹¤. 


## 12. ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥


```python
save_files = {
    f'{OUTPUT_PREFIX}_ìµœì¢…ëª¨ë¸.pkl': final_model,
    f'{OUTPUT_PREFIX}_ì„ê³„ê°’.pkl': {'selected': selected_threshold, 'red': red_threshold, 'yellow': yellow_threshold},
    f'{OUTPUT_PREFIX}_ê²°ê³¼.pkl': {'model_name': final_name, 'test_pr_auc': test_pr_auc, 'test_recall': test_recall, 'test_f2': test_f2}
}

for fname, data in save_files.items():
    joblib.dump(data, os.path.join(PROCESSED_DIR, fname))
    print(f'âœ… {fname}')

print(f'\nì €ì¥ ìœ„ì¹˜: {PROCESSED_DIR}')
```

    âœ… ë°œí‘œ_Part3_v3_ìµœì¢…ëª¨ë¸.pkl
    âœ… ë°œí‘œ_Part3_v3_ì„ê³„ê°’.pkl
    âœ… ë°œí‘œ_Part3_v3_ê²°ê³¼.pkl
    
    ì €ì¥ ìœ„ì¹˜: /Users/user/Desktop/ì•ˆì•Œë´ì¥¼/data/processed


## 13. ìµœì¢… ìš”ì•½

### âœ… v3 ê°œì„ ì‚¬í•­ ë° ì‹¤í—˜ ê²°ê³¼

**Priority 1: ëª¨ë¸ ê³ ë„í™”**
- **Ensemble ê²€ì¦:** Wilcoxon Test ê²°ê³¼ **p-value 0.0625** ($\ge$ 0.05)ë¡œ ë‹¨ì¼ ëª¨ë¸ê³¼ ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—†ìŒ.
    - $\rightarrow$ **CatBoost (Single)** ìµœì¢… ì„ ì • (ìœ ì§€ë³´ìˆ˜ ë° ì„¤ëª…ë ¥ ìš°ìœ„).
- **AutoML:** íƒìƒ‰ íšŸìˆ˜ í™•ì¥ (`n_iter` 30 $\rightarrow$ 50íšŒ ìˆ˜í–‰).

**Priority 2: ìµœì í™” ë° ì‹œê°í™”**
- **Robust ì„ê³„ê°’:** Validation(0.0940)ê³¼ CV(0.0772)ì˜ í‰ê· ì¸ **0.0856**ë¡œ í™•ì •.
- **ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œê°í™”:** Cumulative Gains Curve, Traffic Light Risk ë“±ê¸‰, PR Curve ë“± ì™„ë£Œ.

**Priority 3: ì „ì²˜ë¦¬ ë° ì§„ë‹¨**
- **Winsorizer ì‹¤í—˜:** ì ìš© ì‹œ ì„±ëŠ¥ ì†Œí­ í•˜ë½(0.1284 $\rightarrow$ 0.1188)í•˜ì—¬ **ë¯¸ì ìš©(False)** ê²°ì •.
- **ëª¨ë¸ ì§„ë‹¨:** Calibration Curve(í™•ë¥  ì •í•©ì„±) ë° Learning Curve(í•™ìŠµ ì•ˆì •ì„±) ì–‘í˜¸.

### ğŸ¯ ìµœì¢… í•µì‹¬ ì„±ê³¼ (Test Set í‰ê°€)

âœ… **Data Leakage ì™„ì „ ì œê±°:** ìˆœìˆ˜ ì¬ë¬´ ì§€í‘œ ê¸°ë°˜ì˜ ê±´ì „í•œ ëª¨ë¸ êµ¬ì¶•.

âœ… **ì¡°ê¸° ê²½ë³´ ëŠ¥ë ¥ ì…ì¦:** ì‹¤ì œ ë¶€ë„ ê¸°ì—…ì˜ **71.71% (Recall)** ì‚¬ì „ íƒì§€ ì„±ê³µ.

âœ… **ì˜ˆì¸¡ íš¨ìœ¨ì„± í™•ë³´:** PR-AUC **0.1033** ë‹¬ì„± (Baseline 1.52% ëŒ€ë¹„ 6.8ë°° ì´ìƒ).

âœ… **ìš´ì˜ ì‹ ë¢°ì„± í™•ë³´:** í†µê³„ì  ê²€ì¦ì„ ê±°ì¹œ ëª¨ë¸ê³¼ ë³´ìˆ˜ì  ì„ê³„ê°’(8.56%) ì ìš©.

---

**ë…¸íŠ¸ë¶ ì™„ë£Œ!** ğŸ‰


